<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Kafka消息重复消费]]></title>
    <url>%2F2019%2F01%2F22%2Fkafka-message-re-consumed%2F</url>
    <content type="text"><![CDATA[问题现象最近在测试环境遇到了一个问题, 这两天收到了无缘无故的收到了大批测试短信, 而且都是几天前已经收到过的短信. 经过追踪日志发现, 短息发送是通过Kafka Consumer执行, 而Kafka Consumer把之前的消费了的消息, 又重新消费了一遍.经过资料, 最终在Kafka官文发现了点端倪. 通过阅读官文(KIP-186: Increase offsets retention default to 7 days), 其说明在offsets.retention小于log.retention的时候, 会出现Offset丢失的情况. 官文提示: Offset retention should be always greater than log retention. 问题重现通过以下步骤重现了此问题: 更改配置 变更原始配置修改之后的配置更改Kafka Server配置, 将offsets.retention.minutes从1440改为10. 这样, 日志保留时长(log.retention.hours)为7天, Offset保留时长为10分钟. Offset保留时时长短, 夜里与测试.KafkaConfig values: advertised.host.name = null advertised.listeners = null advertised.port = null alter.config.policy.class.name = null alter.log.dirs.replication.quota.window.num = 11 alter.log.dirs.replication.quota.window.size.seconds = 1 authorizer.class.name = auto.create.topics.enable = true auto.leader.rebalance.enable = true background.threads = 10 broker.id = 0 broker.id.generation.enable = true broker.rack = null compression.type = producer connections.max.idle.ms = 600000 controlled.shutdown.enable = true controlled.shutdown.max.retries = 3 controlled.shutdown.retry.backoff.ms = 5000 controller.socket.timeout.ms = 30000 create.topic.policy.class.name = null default.replication.factor = 1 delegation.token.expiry.check.interval.ms = 3600000 delegation.token.expiry.time.ms = 86400000 delegation.token.master.key = null delegation.token.max.lifetime.ms = 604800000 delete.records.purgatory.purge.interval.requests = 1 delete.topic.enable = true fetch.purgatory.purge.interval.requests = 1000 group.initial.rebalance.delay.ms = 0 group.max.session.timeout.ms = 300000 group.min.session.timeout.ms = 6000 host.name = inter.broker.listener.name = null inter.broker.protocol.version = 1.1-IV0 leader.imbalance.check.interval.seconds = 300 leader.imbalance.per.broker.percentage = 10 listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL listeners = null log.cleaner.backoff.ms = 15000 log.cleaner.dedupe.buffer.size = 134217728 log.cleaner.delete.retention.ms = 86400000 log.cleaner.enable = true log.cleaner.io.buffer.load.factor = 0.9 log.cleaner.io.buffer.size = 524288 log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308 log.cleaner.min.cleanable.ratio = 0.5 log.cleaner.min.compaction.lag.ms = 0 log.cleaner.threads = 1 log.cleanup.policy = [delete] log.dir = /tmp/kafka-logs log.dirs = /tmp/kafka-logs log.flush.interval.messages = 9223372036854775807 log.flush.interval.ms = null log.flush.offset.checkpoint.interval.ms = 60000 log.flush.scheduler.interval.ms = 9223372036854775807 log.flush.start.offset.checkpoint.interval.ms = 60000 log.index.interval.bytes = 4096 log.index.size.max.bytes = 10485760 log.message.format.version = 1.1-IV0 log.message.timestamp.difference.max.ms = 9223372036854775807 log.message.timestamp.type = CreateTime log.preallocate = false log.retention.bytes = -1 log.retention.check.interval.ms = 300000 log.retention.hours = 168 log.retention.minutes = null log.retention.ms = null log.roll.hours = 168 log.roll.jitter.hours = 0 log.roll.jitter.ms = null log.roll.ms = null log.segment.bytes = 1073741824 log.segment.delete.delay.ms = 60000 max.connections.per.ip = 2147483647 max.connections.per.ip.overrides = max.incremental.fetch.session.cache.slots = 1000 message.max.bytes = 1000012 metric.reporters = [] metrics.num.samples = 2 metrics.recording.level = INFO metrics.sample.window.ms = 30000 min.insync.replicas = 1 num.io.threads = 8 num.network.threads = 3 num.partitions = 1 num.recovery.threads.per.data.dir = 1 num.replica.alter.log.dirs.threads = null num.replica.fetchers = 1 offset.metadata.max.bytes = 4096 offsets.commit.required.acks = -1 offsets.commit.timeout.ms = 5000 offsets.load.buffer.size = 5242880 offsets.retention.check.interval.ms = 600000 offsets.retention.minutes = 1440 offsets.topic.compression.codec = 0 offsets.topic.num.partitions = 50 offsets.topic.replication.factor = 1 offsets.topic.segment.bytes = 104857600 password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding password.encoder.iterations = 4096 password.encoder.key.length = 128 password.encoder.keyfactory.algorithm = null password.encoder.old.secret = null password.encoder.secret = null port = 9092 principal.builder.class = null producer.purgatory.purge.interval.requests = 1000 queued.max.request.bytes = -1 queued.max.requests = 500 quota.consumer.default = 9223372036854775807 quota.producer.default = 9223372036854775807 quota.window.num = 11 quota.window.size.seconds = 1 replica.fetch.backoff.ms = 1000 replica.fetch.max.bytes = 1048576 replica.fetch.min.bytes = 1 replica.fetch.response.max.bytes = 10485760 replica.fetch.wait.max.ms = 500 replica.high.watermark.checkpoint.interval.ms = 5000 replica.lag.time.max.ms = 10000 replica.socket.receive.buffer.bytes = 65536 replica.socket.timeout.ms = 30000 replication.quota.window.num = 11 replication.quota.window.size.seconds = 1 request.timeout.ms = 30000 reserved.broker.max.id = 1000 sasl.enabled.mechanisms = [GSSAPI] sasl.jaas.config = null sasl.kerberos.kinit.cmd = /usr/bin/kinit sasl.kerberos.min.time.before.relogin = 60000 sasl.kerberos.principal.to.local.rules = [DEFAULT] sasl.kerberos.service.name = null sasl.kerberos.ticket.renew.jitter = 0.05 sasl.kerberos.ticket.renew.window.factor = 0.8 sasl.mechanism.inter.broker.protocol = GSSAPI security.inter.broker.protocol = PLAINTEXT socket.receive.buffer.bytes = 102400 socket.request.max.bytes = 104857600 socket.send.buffer.bytes = 102400 ssl.cipher.suites = [] ssl.client.auth = none ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1] ssl.endpoint.identification.algorithm = null ssl.key.password = null ssl.keymanager.algorithm = SunX509 ssl.keystore.location = null ssl.keystore.password = null ssl.keystore.type = JKS ssl.protocol = TLS ssl.provider = null ssl.secure.random.implementation = null ssl.trustmanager.algorithm = PKIX ssl.truststore.location = null ssl.truststore.password = null ssl.truststore.type = JKS transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000 transaction.max.timeout.ms = 900000 transaction.remove.expired.transaction.cleanup.interval.ms = 3600000 transaction.state.log.load.buffer.size = 5242880 transaction.state.log.min.isr = 1 transaction.state.log.num.partitions = 50 transaction.state.log.replication.factor = 1 transaction.state.log.segment.bytes = 104857600 transactional.id.expiration.ms = 604800000 unclean.leader.election.enable = false zookeeper.connect = localhost:2181 zookeeper.connection.timeout.ms = 6000 zookeeper.max.in.flight.requests = 10 zookeeper.session.timeout.ms = 6000 zookeeper.set.acl = false zookeeper.sync.time.ms = 2000 KafkaConfig values: advertised.host.name = null advertised.listeners = null advertised.port = null alter.config.policy.class.name = null alter.log.dirs.replication.quota.window.num = 11 alter.log.dirs.replication.quota.window.size.seconds = 1 authorizer.class.name = auto.create.topics.enable = true auto.leader.rebalance.enable = true background.threads = 10 broker.id = 0 broker.id.generation.enable = true broker.rack = null compression.type = producer connections.max.idle.ms = 600000 controlled.shutdown.enable = true controlled.shutdown.max.retries = 3 controlled.shutdown.retry.backoff.ms = 5000 controller.socket.timeout.ms = 30000 create.topic.policy.class.name = null default.replication.factor = 1 delegation.token.expiry.check.interval.ms = 3600000 delegation.token.expiry.time.ms = 86400000 delegation.token.master.key = null delegation.token.max.lifetime.ms = 604800000 delete.records.purgatory.purge.interval.requests = 1 delete.topic.enable = true fetch.purgatory.purge.interval.requests = 1000 group.initial.rebalance.delay.ms = 0 group.max.session.timeout.ms = 300000 group.min.session.timeout.ms = 6000 host.name = inter.broker.listener.name = null inter.broker.protocol.version = 1.1-IV0 leader.imbalance.check.interval.seconds = 300 leader.imbalance.per.broker.percentage = 10 listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL listeners = null log.cleaner.backoff.ms = 15000 log.cleaner.dedupe.buffer.size = 134217728 log.cleaner.delete.retention.ms = 86400000 log.cleaner.enable = true log.cleaner.io.buffer.load.factor = 0.9 log.cleaner.io.buffer.size = 524288 log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308 log.cleaner.min.cleanable.ratio = 0.5 log.cleaner.min.compaction.lag.ms = 0 log.cleaner.threads = 1 log.cleanup.policy = [delete] log.dir = /tmp/kafka-logs log.dirs = /tmp/kafka-logs log.flush.interval.messages = 9223372036854775807 log.flush.interval.ms = null log.flush.offset.checkpoint.interval.ms = 60000 log.flush.scheduler.interval.ms = 9223372036854775807 log.flush.start.offset.checkpoint.interval.ms = 60000 log.index.interval.bytes = 4096 log.index.size.max.bytes = 10485760 log.message.format.version = 1.1-IV0 log.message.timestamp.difference.max.ms = 9223372036854775807 log.message.timestamp.type = CreateTime log.preallocate = false log.retention.bytes = -1 log.retention.check.interval.ms = 300000 log.retention.hours = 168 log.retention.minutes = null log.retention.ms = null log.roll.hours = 168 log.roll.jitter.hours = 0 log.roll.jitter.ms = null log.roll.ms = null log.segment.bytes = 1073741824 log.segment.delete.delay.ms = 60000 max.connections.per.ip = 2147483647 max.connections.per.ip.overrides = max.incremental.fetch.session.cache.slots = 1000 message.max.bytes = 1000012 metric.reporters = [] metrics.num.samples = 2 metrics.recording.level = INFO metrics.sample.window.ms = 30000 min.insync.replicas = 1 num.io.threads = 8 num.network.threads = 3 num.partitions = 1 num.recovery.threads.per.data.dir = 1 num.replica.alter.log.dirs.threads = null num.replica.fetchers = 1 offset.metadata.max.bytes = 4096 offsets.commit.required.acks = -1 offsets.commit.timeout.ms = 5000 offsets.load.buffer.size = 5242880 offsets.retention.check.interval.ms = 600000 offsets.retention.minutes = 10 offsets.topic.compression.codec = 0 offsets.topic.num.partitions = 50 offsets.topic.replication.factor = 1 offsets.topic.segment.bytes = 104857600 password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding password.encoder.iterations = 4096 password.encoder.key.length = 128 password.encoder.keyfactory.algorithm = null password.encoder.old.secret = null password.encoder.secret = null port = 9092 principal.builder.class = null producer.purgatory.purge.interval.requests = 1000 queued.max.request.bytes = -1 queued.max.requests = 500 quota.consumer.default = 9223372036854775807 quota.producer.default = 9223372036854775807 quota.window.num = 11 quota.window.size.seconds = 1 replica.fetch.backoff.ms = 1000 replica.fetch.max.bytes = 1048576 replica.fetch.min.bytes = 1 replica.fetch.response.max.bytes = 10485760 replica.fetch.wait.max.ms = 500 replica.high.watermark.checkpoint.interval.ms = 5000 replica.lag.time.max.ms = 10000 replica.socket.receive.buffer.bytes = 65536 replica.socket.timeout.ms = 30000 replication.quota.window.num = 11 replication.quota.window.size.seconds = 1 request.timeout.ms = 30000 reserved.broker.max.id = 1000 sasl.enabled.mechanisms = [GSSAPI] sasl.jaas.config = null sasl.kerberos.kinit.cmd = /usr/bin/kinit sasl.kerberos.min.time.before.relogin = 60000 sasl.kerberos.principal.to.local.rules = [DEFAULT] sasl.kerberos.service.name = null sasl.kerberos.ticket.renew.jitter = 0.05 sasl.kerberos.ticket.renew.window.factor = 0.8 sasl.mechanism.inter.broker.protocol = GSSAPI security.inter.broker.protocol = PLAINTEXT socket.receive.buffer.bytes = 102400 socket.request.max.bytes = 104857600 socket.send.buffer.bytes = 102400 ssl.cipher.suites = [] ssl.client.auth = none ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1] ssl.endpoint.identification.algorithm = null ssl.key.password = null ssl.keymanager.algorithm = SunX509 ssl.keystore.location = null ssl.keystore.password = null ssl.keystore.type = JKS ssl.protocol = TLS ssl.provider = null ssl.secure.random.implementation = null ssl.trustmanager.algorithm = PKIX ssl.truststore.location = null ssl.truststore.password = null ssl.truststore.type = JKS transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000 transaction.max.timeout.ms = 900000 transaction.remove.expired.transaction.cleanup.interval.ms = 3600000 transaction.state.log.load.buffer.size = 5242880 transaction.state.log.min.isr = 1 transaction.state.log.num.partitions = 50 transaction.state.log.replication.factor = 1 transaction.state.log.segment.bytes = 104857600 transactional.id.expiration.ms = 604800000 unclean.leader.election.enable = false zookeeper.connect = localhost:2181 zookeeper.connection.timeout.ms = 6000 zookeeper.max.in.flight.requests = 10 zookeeper.session.timeout.ms = 6000 zookeeper.set.acl = false zookeeper.sync.time.ms = 2000 在2019-01-22 10:08:40.220时, Producer发送了一个消息 12019-01-22 10:08:40.220 [INFO] The message#[topic=event-start-strategy-treatment, key=2d442718, offset=60] is sent to partition#0 at Tue Jan 22 10:08:40 CST 2019 successfully. 在2019-01-22 10:08:40.495时, Consumer处理了此消息, 并提交了Offset 12019-01-22 10:08:40.495 [DEBUG] Group hrx-strategy-treatment-action committed offset 62 for partition event-start-strategy-treatment-0 此时Kafka Topic 的Offset为62 12$: bin/kafka-run-class.sh kafka.tools.GetOffsetShell --broker-list localhost:9092 --topic event-start-strategy-treatment event-start-strategy-treatment:0:62 在2019-01-22 10:34:20,471的时候, Kafka Server清除了Offset 123[2019-01-22 10:24:20,489] INFO [GroupMetadataManager brokerId=0] Removed 0 expired offsets in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)[2019-01-22 10:34:20,471] INFO [GroupMetadataManager brokerId=0] Removed 2 expired offsets in 2 milliseconds. (kafka.coordinator.group.GroupMetadataManager)[2019-01-22 10:44:20,447] INFO [GroupMetadataManager brokerId=0] Removed 0 expired offsets in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager) 注意: 此时Consumer端没有任何反应 在2019-01-22 10:48:26.596时, 重启consumer, 此时consumer将Topic中的62条消息全部重新消费. 至此, Kafka重新消费消息的问题重现. 结论Kafka会定时清除日志(消息)和Offsets, 如果Offset丢失了, 而日志还保留了, 这些日志的会被同步到Offset记录中, 并且被标记为未消费的内容, 故这些日志(消息)会被Kafka Consumer重新Pull并消费. 结论: Offset retention should be always greater than log retention. Offset的保留时长应当大于日志(消息)的保留时长. 解决方案: 应当参考KIP-186: Increase offsets retention default to 7 days, 合理配置日志和Offset的保留时长. 新版的Kafka Server不用担心此问题, 因为KIP-186的解决, Kafka Server的默认Offset的保留时长已经修改为7天了.]]></content>
      <categories>
        <category>消息中间件</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka常用命令]]></title>
    <url>%2F2018%2F10%2F18%2Fkafka-cmd%2F</url>
    <content type="text"><![CDATA[Kafka命令行工具 查看Kafka现有的Topic 1bin/kafka-topics.sh --zookeeper localhost:2181 --list 查看Topic详情 1bin/kafka-topics.sh --zookeeper localhost:2181 --describe --topic my-topic 配置Kafka现有的Topic的Partition 1bin/kafka-topics.sh --zookeeper localhost:2181 --alter --topic my-topic --partitions 3 清除Topic的数据 12bin/kafka-topics.sh --zookeeper localhost:2181 --alter --topic my-topic --config retention.ms=1000bin/kafka-topics.sh --zookeeper localhost:2181 --alter --topic my-topic --delete-config retention.ms 删除Topic 1bin/kafka-topics.sh --zookeeper localhost:2181 --delete --topic my-topic 查看Topic的Offsetbin/kafka-run-class.sh kafka.tools.GetOffsetShell –broker-list localhost:9092 –topic my-topic 查看Topic的消息数量1bin/kafka-run-class.sh kafka.tools.GetOffsetShell --broker-list localhost:9092 --topic my-topic --time -1 --offsets 1 | awk -F &quot;:&quot; &apos;&#123;sum += $3&#125; END &#123;print sum&#125;&apos; 1bin/kafka-console-consumer.sh config/consumer.properties --from-beginning --topic __consumer_offsets --bootstrap-server localhost:9092 --formatter &quot;kafka.coordinator.group.GroupMetadataManager\$OffsetsMessageFormatter&quot;]]></content>
      <categories>
        <category>消息中间件</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka使用中遇到的问题]]></title>
    <url>%2F2018%2F07%2F04%2Fkafka-my-problem%2F</url>
    <content type="text"><![CDATA[Hostname问题在Kafka的启动的时候遇到以下报错(包括kafka Test启动):1234567891011java.lang.NoClassDefFoundError: Could not initialize class kafka.network.RequestChannel$ at kafka.network.RequestChannel$Request.&lt;init&gt;(RequestChannel.scala:114) ~[kafka_2.11-0.10.1.1.jar:na] at kafka.network.Processor$$anonfun$processCompletedReceives$1.apply(SocketServer.scala:492) ~[kafka_2.11-0.10.1.1.jar:na] at kafka.network.Processor$$anonfun$processCompletedReceives$1.apply(SocketServer.scala:487) ~[kafka_2.11-0.10.1.1.jar:na] at scala.collection.Iterator$class.foreach(Iterator.scala:893) ~[scala-library-2.11.8.jar:na] at scala.collection.AbstractIterator.foreach(Iterator.scala:1336) ~[scala-library-2.11.8.jar:na] at scala.collection.IterableLike$class.foreach(IterableLike.scala:72) ~[scala-library-2.11.8.jar:na] at scala.collection.AbstractIterable.foreach(Iterable.scala:54) ~[scala-library-2.11.8.jar:na] at kafka.network.Processor.processCompletedReceives(SocketServer.scala:487) ~[kafka_2.11-0.10.1.1.jar:na] at kafka.network.Processor.run(SocketServer.scala:417) ~[kafka_2.11-0.10.1.1.jar:na] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_144] 经过反复测试需要配置主机的hostname有在host里头配置 第一步: 通过hostname命令查看主机hostname: 12&gt; hostnamemy.local 在/etc/hosts中配置hostname的映射 1127.0.0.1 my.local]]></content>
      <categories>
        <category>消息中间件</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DynamoDB集成EMR]]></title>
    <url>%2F2018%2F04%2F08%2Faws-dynamodb-5-emr%2F</url>
    <content type="text"><![CDATA[配置 DynamoDB权限配置 本地测试需要给当前用户的group配置DynamoDB的访问权限, Lamdba和EC2可以通过VPC来访问 配置AWS Token 12345AWS.config.update(&#123; accessKeyId: 'xxx', secretAccessKey: 'xxx', region: 'us-east-1'&#125;); 建模 City表: 1234567891011121314151617181920212223242526272829303132333435363738&#123; name: 'City', schema: &#123; fields: &#123; id: &#123; type: Number, hashKey: true &#125;, city: &#123; type: String, rangeKey: true &#125;, zip: &#123; type: String, index: &#123; global: true, rangeKey: 'city', name: 'ZipNameIndex', // project: ['creationDate'], // ProjectionType: INCLUDE throughput: 5 // read and write are both 5 &#125; &#125; &#125;, config: &#123; throughput: &#123; // Sets the throughput of the DynamoDB table on creation read: 5, write: 5 &#125;, useNativeBooleans: true, // Store Boolean values as Boolean (‘BOOL’) in DynamoDB useDocumentTypes: true, //Store Objects and Arrays as Maps (‘M’) and Lists (‘L’) types in DynamoDB. saveUnknown: true, // Specifies that attributes not defined in the schema will be saved and retrieved. This defaults to false. // timestamps: &#123; // Defines that schema must contain fields to control creation and last update timestamps. If it is set to true, this fields will be createdAt for creation date and updatedAt for last update // createdAt: 'creationDate', // updatedAt: 'lastUpdateDate' // &#125; &#125; &#125;&#125; User表: 1234567891011121314151617181920212223242526272829303132&#123; name: 'User', schema: &#123; fields: &#123; username: &#123; type: String, hashKey: true &#125;, email: &#123; type: String, rangeKey: true, &#125;, firstName: String, lastName: String, dateOfBirth: Date, city: Number &#125;, config: &#123; throughput: &#123; // Sets the throughput of the DynamoDB table on creation read: 10, write: 10 &#125;, useNativeBooleans: true, // Store Boolean values as Boolean (‘BOOL’) in DynamoDB useDocumentTypes: true, //Store Objects and Arrays as Maps (‘M’) and Lists (‘L’) types in DynamoDB. saveUnknown: true, // Specifies that attributes not defined in the schema will be saved and retrieved. This defaults to false. timestamps: &#123; // Defines that schema must contain fields to control creation and last update timestamps. If it is set to true, this fields will be createdAt for creation date and updatedAt for last update createdAt: 'creationDate', updatedAt: 'lastUpdateDate' &#125; &#125; &#125;&#125; 创建表病导入数据 通过Test Case创建表, 并将mock-data.js产生的测试数据导入到数据库 1234567891011121314151617181920212223242526272829it('City record should be batch added successfully', () =&gt; &#123; var allCities = JSON.parse(fs.readFileSync(normalPath + '/mock-data-cities.json', 'utf8')).data; expect(allCities.length).to.be.equal(10); var items = []; allCities.forEach(p =&gt; &#123; items.push(new City(p)); &#125;); return City.batchPut(items).then(data =&gt; &#123; expect(Object.keys(data.Responses)).to.have.lengthOf(0); expect(Object.keys(data.UnprocessedItems)).to.have.lengthOf(0); &#125;);&#125;);it('User record should be batch added successfully', () =&gt; &#123; var allUsers = JSON.parse(fs.readFileSync(normalPath + '/mock-data-users.json', 'utf8')).data; expect(allUsers.length).to.be.equal(200); var items = []; allUsers.forEach(p =&gt; &#123; items.push(new User(p)); &#125;); return User.batchPut(items).then(data =&gt; &#123; expect(Object.keys(data.Responses)).to.have.lengthOf(0); expect(Object.keys(data.UnprocessedItems)).to.have.lengthOf(0); &#125;);&#125;); 在 DynamoDB Console 检查数据 Amazon EMRHadoop: 分布式文件系统 + MapReduceHive: 构建在Hadoop之上的数据仓库工具, 提供了简单的结构化查询语言, 可以将sql语句转换为MapReduce任务进行运行. 启动Hive集群准备密钥对 (可选) 进入 EC2 Console, 配置密钥对 进入 Amazon EMR Console, 创建Hadoop集群 配置并启动Hadoop集群 注意配置密钥对, 后续测试相关: 查看Hadoop集群详情 集群详情 EC2 集群详情 访问集成集群 (可选) Hadoop集群已经启动, 我们可以通过SSH访问Hadoop 配置EC2的Security Group, 保证一些服务的端口可用, 比如ssh对应的20端口 通过SSH命令访问 1ssh -i KAT-ElastiCacheEC2.pem hadoop@ec2-35-172-190-32.compute-1.amazonaws.com 测试Hive命令: DynamoDB集成Hive使用 HiveQL 访问外部数据源(DynamoDB中存储的数据), 帮助我们处理并分析数据源 本地开发环境搭建配置SSH 隧道 因为EC2没有配置Public IP, 所以我们通过SSH 隧道以方便在本地访问Hadoop集群: 1ssh -o ServerAliveInterval=10 -i path-to-key-file -N -L 10000:localhost:10000 hadoop@master-public-dns-name ==&gt; 1ssh -o ServerAliveInterval=10 -i KAT-ElastiCacheEC2.pem -N -L 10000:localhost:10000 hadoop@ec2-35-172-190-32.compute-1.amazonaws.com 通过SSH 隧道将Hadoop集群master节点的10000端口映射到本地的10000端口上. 测试SSH 隧道 使用JDBC访问Hive. 可以通SQL Workbench/J来测试, 参考: 使用 Hive JDBC 驱动程序 Java程序 Java程序通过JDBC连接Hive下载Amazon Hive Jdbc Driver 下载: https://docs.aws.amazon.com/zh_cn/emr/latest/ReleaseGuide/HiveJDBCDriver.html 加载Amazon Hive Jdbc Driver pom文件: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556&lt;!--START local amazon hive jdbc driver dependencies. They are not in maven central/finra CM repo.--&gt;&lt;dependency&gt; &lt;groupId&gt;com.amazon.hive&lt;/groupId&gt; &lt;artifactId&gt;hive_service&lt;/artifactId&gt; &lt;version&gt;1.0&lt;/version&gt; &lt;scope&gt;system&lt;/scope&gt; &lt;systemPath&gt;$&#123;project.basedir&#125;/src/main/resources/libs/hive_service.jar&lt;/systemPath&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.amazon.hive&lt;/groupId&gt; &lt;artifactId&gt;HiveJDBC41&lt;/artifactId&gt; &lt;version&gt;1.0&lt;/version&gt; &lt;scope&gt;system&lt;/scope&gt; &lt;systemPath&gt;$&#123;project.basedir&#125;/src/main/resources/libs/HiveJDBC41.jar&lt;/systemPath&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.amazon.hive&lt;/groupId&gt; &lt;artifactId&gt;libfb303&lt;/artifactId&gt; &lt;version&gt;1.0&lt;/version&gt; &lt;scope&gt;system&lt;/scope&gt; &lt;systemPath&gt;$&#123;project.basedir&#125;/src/main/resources/libs/libfb303-0.9.0.jar&lt;/systemPath&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.amazon.hive&lt;/groupId&gt; &lt;artifactId&gt;libthrift&lt;/artifactId&gt; &lt;version&gt;1.0&lt;/version&gt; &lt;scope&gt;system&lt;/scope&gt; &lt;systemPath&gt;$&#123;project.basedir&#125;/src/main/resources/libs/libthrift-0.9.0.jar&lt;/systemPath&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.amazon.hive&lt;/groupId&gt; &lt;artifactId&gt;ql&lt;/artifactId&gt; &lt;version&gt;1.0&lt;/version&gt; &lt;scope&gt;system&lt;/scope&gt; &lt;systemPath&gt;$&#123;project.basedir&#125;/src/main/resources/libs/ql.jar&lt;/systemPath&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.amazon.hive&lt;/groupId&gt; &lt;artifactId&gt;TCLIServiceClient&lt;/artifactId&gt; &lt;version&gt;1.0&lt;/version&gt; &lt;scope&gt;system&lt;/scope&gt; &lt;systemPath&gt;$&#123;project.basedir&#125;/src/main/resources/libs/TCLIServiceClient.jar&lt;/systemPath&gt;&lt;/dependency&gt;&lt;!--END local amazon hive jdbc driver dependencies--&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.httpcomponents&lt;/groupId&gt; &lt;artifactId&gt;httpclient&lt;/artifactId&gt; &lt;version&gt;4.1.3&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.zookeeper&lt;/groupId&gt; &lt;artifactId&gt;zookeeper&lt;/artifactId&gt; &lt;version&gt;3.4.6&lt;/version&gt; &lt;!--&lt;type&gt;pom&lt;/type&gt;--&gt;&lt;/dependency&gt; Load Driver Class 123456789private static String driverName = "com.amazon.hive.jdbc41.HS2Driver";try &#123; Class.forName(driverName);&#125; catch (ClassNotFoundException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); System.exit(1);&#125; 连接Hive和测试 创建连接 12345678protected Connection con;/** TODO: DOCUMENT ME! */protected Statement stmt;con = DriverManager.getConnection(JDBC_URL, "hadoop", "");stmt = con.createStatement(); 执行SQL语句: 123@Test public void test1DropTable() throws SQLException &#123; stmt.execute("DROP TABLE IF EXISTS MY_Table");&#125; DynamoDB和Hive的表的映射将 DynamoDB 表导出到 HDFS 通过Hive命令创建Hive外部表, 将DynamoDB的表映射Hive表中. 1234CREATE EXTERNAL TABLE hiveTableName (col1 string, col2 bigint, col3 array&lt;string&gt;)STORED BY 'org.apache.hadoop.hive.dynamodb.DynamoDBStorageHandler'TBLPROPERTIES ("dynamodb.table.name" = "dynamodbtable1","dynamodb.column.mapping" = "col1:name,col2:year,col3:holidays"); ==&gt; 12345678CREATE EXTERNAL TABLE HiveCity (city_id BIGINT, city_name STRING, post_code STRING)STORED BY 'org.apache.hadoop.hive.dynamodb.DynamoDBStorageHandler'TBLPROPERTIES( "dynamodb.table.name" = "KAT-RES-City", "dynamodb.column.mapping"="city_id:id,city_name:city,post_code:zip"); 映射表创建成功后, DynamoDB中的数据会及时同步到HDFS中. 查询Hive表 1select * from HiveCity; Hive表的做统计AWS EMR的select count(*)默认是不工作的(参考 Amazon EMR的已知问题 ). 统计有以下两种实现方式: 通过Hive命令ANALYZE TABLE来做( 参考 ), 比如: ANALYZE TABLE HiveCity COMPUTE STATISTICS.这个命令是执行一个MapReduce的Job来处理数据, 运行了ANALYZE命令后, count(*)才会得到正确的值. 注意: ANALYZE命令一次有效, 当DynamoDB的数据有增删的话, 再次运行count(*)是不会会得到正确的值(还是上次ANALYZE的结果). 通过配置set hive.compute.query.using.stats=falseAmazon EMR默认将其设置为true. 当关闭hive.compute.query.using.stats时, Hive的每条统计查询语句会转换成一条MapReduce的Job来执行(普通的查询语句不会转换).这样的count(*)每次都会的到最新的结果. 多Hive表的俩和查询由于DynamoDB不支持连接查询, 我们将DynamoDB的映射为Hive的多张表, 通过HiveQL的, 我们就可以做到DynamoDB的多表联合查询. 比如这条联合查询语句: 1select u.user_name, c.city_name from HiveUser u join HiveCity c on u.city_id = c.city_id Hive将其转换成为MapReduce的Job来执行. 更多的查询语句: 查询 DynamoDB 表中的数据 Hive将数据复制到DynamoDBHive也可以将处理的后的数据在写入到Hive的DynamoDB的映射表, 相当于Hive将数据写入到DynamoDB中, 如下: 12INSERT OVERWRITE TABLE User_SNAPSHOTselect * from HiveUser where city_id = 1; 注意事项 如果JDBC执行HiveQL产生了MapReduce的Job(比如count(*), select … join …), JDBC连接必须配置用户名密码, 用户名为hadoop, 密码为空. 1DriverManager.getConnection(JDBC_URL, "hadoop", ""); DynamoDB的吞吐量: Hive同步数据到DynamoDB会消耗DynamoDB的吞吐量. 我们需要根据实际的数据量大小来平衡同步速率和吞吐量 (参考 DynamoDB 配置的吞吐量 ) 数据类型映射问题: DynamoDB和Hive两个的数据类型不是完全匹配的, 不是所有的字段都可以映射到Hive上的( 参考 ) Hive有的查询语句会转换成为MapReduce的Job, 他不会马上返回结果, 需要等待Job执行完成. 这个过程在5s以上 在NodeJs里访问Hive的问题: 目前在NodeJS里头访问Hive是可以使用JDBC Driver做到的, 但是它需要JDK运行时, 所以在AWS Lambda - NodeJS上无法实现访问Hive.]]></content>
      <tags>
        <tag>DynamoDB</tag>
        <tag>AWS</tag>
        <tag>EMR</tag>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Amazon ElastiCache for Redis in Lambda]]></title>
    <url>%2F2018%2F04%2F03%2Faws-elasti-cache-lambda%2F</url>
    <content type="text"><![CDATA[Configure VPCAs there is no auth to connect to Redis server, all security policy are based on VPC‘s Security Group, so before setup Redis server, we have to configure the VPC. Sign in to VPC Console, make sure there is at least one VPC Open Security Group, create a new Security GroupNote: The VPC should be used same one in the following configuration Launch a Redis ClusterAmazon ElastiCache for Redis clusters provide a variety of node configurations: Single Node Single Shard Multiple NodesA single shard configuration groups up to 6 nodes in a single cluster, a read/write Primary node and up to 5 read-only Replica nodes. Multiple Shards Multiple NodesA multiple shard configuration partitions your data across up to 20 shards in the cluster. Each shard has a read/write Primary node and up to 5 read-only replica nodes. Now we setup a ‘Single Node’ Redis Cluster: Sign in to ElastiCache Console Choose Get Started Now. Choose the region Choose Redis as cluster engine Complete ‘Redis settings’Note: As we are configuring a Single Node Redis, so we set Number of Replicas as None. Complete ‘Advanced Redis settings’Note: Keep VPC ID same as the VPC configuration, and remember the Subnets The other items are kept as default Choose Create cluster to launch your clusterThe Create operation will take a while. View the details of cluster cluster we createdPrimary Endpoint will be configured Redis client for connecting Redis server. Click on the cluster’s name, the node(s) in the cluster will be listed, and you can see the metrics of each node The main step is checking the Security Group, make sure the its same as we created: Now the Redis Server on AWS is up. Prepare Role to Run Lambda Sign in to IAM Console To create a new Role: Choose AWS Service as role type, and Lambda as service Selected AWSLambdaVPCAccessExecutionRole at step of Attach permissions policiesThis permission is enough for us to connect to Redis server in Lambda function. Named the role Role is created! Lambda FunctionCreate Lambda Function Create a Lambda functionUser Node.js as dev env, and select the role we just created Created Lambda function Configure Lambda Role, min required memory or timeout configuration Configure NetworkNote: VPC must be same as above configuration Subnets must contain the same Subnets when we setup Redis Security Groups must be save as above configuration Write Lambda Function to connect to RedisWe use node_redis as client to connect Redis server in our Lambda function. The project structure is as below: 1234lambda_function|-- node_modules|-- package.json|-- index.js The dependencies in package.json is: 12345"main": "index.js","dependencies": &#123; "bluebird": "^3.5.1", "redis": "^2.8.0"&#125; bluebird is for Promise implementation. The code implementation of Lambda function is in index.js: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106'use strict';var redis = require('redis');var bluebird = require('bluebird');bluebird.promisifyAll(redis.RedisClient.prototype);bluebird.promisifyAll(redis.Multi.prototype);const GLOBAL_KEY = 'lambda-test';const redisOptions = &#123; host: "redistest.cbmmpl.0001.use1.cache.amazonaws.com", port: 6379&#125;redis.debug_mode = true;exports.handler = (event, context, callback) =&gt; &#123; console.info('Start to connect to Redis Server') var client = redis.createClient(redisOptions); console.info('Connected to Redis Server') console.info('event.pathParameters: ', event.pathParameters); console.info('event.httpMethod: ', event.httpMethod); let id = (event.pathParameters || &#123;&#125;).product || false; let data = event.data; switch (event.httpMethod) &#123; case "GET": if (id) &#123; console.info('get by id') client.hgetAsync(GLOBAL_KEY, id).then(res =&gt; &#123; console.info('Redis responses for get single: ', res); callback(null, &#123;body: "This is a READ operation on product ID " + id, ret: res&#125;); // callback(null, &#123;body: "This is a READ operation on product ID " + id&#125;); &#125;).catch(err =&gt; &#123; console.error("Failed to get single: ", err) callback(null, &#123;statusCode: 500, message: "Failed to get data"&#125;); &#125;).finally(() =&gt; &#123; console.info('Disconnect to Redis'); client.quit(); &#125;); return; &#125; else &#123; console.info('get all') client.hgetallAsync(GLOBAL_KEY).then(res =&gt; &#123; console.info('Redis responses for get all: ', res) callback(null, &#123;body: "This is a LIST operation, return all products", ret: res&#125;); // callback(null, &#123;body: "This is a LIST operation, return all products"&#125;); &#125;).catch(err =&gt; &#123; console.error("Failed to post data: ", err) callback(null, &#123;statusCode: 500, message: "Failed to get data"&#125;); &#125;).finally(() =&gt; &#123; console.info('Disconnect to Redis'); client.quit(); &#125;); &#125; break; case "POST": if (data) &#123; console.info('Posting data for [', id, '] with value: ', data); client.hmsetAsync(GLOBAL_KEY, id, data).then(res =&gt; &#123; console.info('Redis responses for post: ', res) callback(null, &#123;body: "This is a CREATE operation and it's successful", ret: res&#125;); // callback(null, &#123;body: "This is a CREATE operation"&#125;); &#125;).catch(err =&gt; &#123; console.error("Failed to post data: ", err) callback(null, &#123;statusCode: 500, message: "Failed to post data"&#125;); &#125;).finally(() =&gt; &#123; console.info('Disconnect to Redis'); client.quit(); &#125;); &#125; else &#123; callback(null, &#123;statusCode: 500, message: 'no data is posted'&#125;) &#125; break; case "PUT": callback(null, &#123;body: "This is an UPDATE operation on product ID " + id&#125;); break; case "DELETE": console.info('delete a prod'); client.delAsync(GLOBAL_KEY).then(res =&gt; &#123; console.info('Redis responses for get single: ', res); callback(null, &#123;body: "This is a DELETE operation on product ID " + id, ret: res&#125;); // callback(null, &#123;body: "This is a DELETE operation on product ID " + id&#125;); &#125;).catch(err =&gt; &#123; console.error("Failed to delete single: ", err); callback(null, &#123;statusCode: 500, message: "Failed to delete data"&#125;); &#125;).finally(() =&gt; &#123; console.info('Disconnect to Redis'); client.quit(); &#125;); break; default: // Send HTTP 501: Not Implemented console.log("Error: unsupported HTTP method (" + event.httpMethod + ")"); callback(null, &#123;statusCode: 501&#125;) &#125;&#125; The redisOptions is from the Redis cluster details. Package the functionFirst install NPM packages: 1npm install Then zip the files. Upload the package: Lambda function is deployed: Test Lambda FunctionTo test the lambda, we need to create some Test Events: Test writing value to Redis Server Create Test Event: 1234567&#123; "httpMethod": "POST", "pathParameters": &#123; "product": "key-1" &#125;, "data": "I am a test string"&#125; Then clicking Test to mock api call: We also can view the logs in AWS CloudWatch Test reading value from Redis Server Create Test Event: 123&#123; "httpMethod": "GET"&#125; Then clicking Test to mock api call:The value we set is populated from Redis server correctly. Access ElastiCache from EC2Excepted AWS Lambda Function, ElastiCache can only be accessed through an Amazon EC2 instance. Below is the step to setup a EC2 instance to access the Redis server: Sign in to EC2 Console Launch a new InstanceWhen configuring the instance details, the Network must set the VPC we created, and Subnet must be same as the above Redis. If the Security Group doesn’t contain the same one as Redis server, you have to change the configuration: Connect the EC2 instance with ssh: Access Redis server through EC2 instance: 1234$ redis-cli -h redistest.yyyy.xxxx.use1.cache.amazonaws.comredistest.yyyy.xxxx.use1.cache.amazonaws.com:6379&gt; pingPONGredistest.yyyy.xxxx.use1.cache.amazonaws.com:6379&gt; When ping responses PONG, it means the EC2 instance has connected to ElastiCache service, now you can deploy/test your application which need Redis service.Note:For this step, the EC2 instance have to be installed redis-cli tool. Scalability of Amazon ElastiCache for RedisThe above example of Redis cluster is a single node, we can add more readonly nodes for it. Add Replication Goto Nodes tab, select the node, then click Add Replication Complete the information, to change the current Single Node cluster to Single Shard and Single Node: Now the changes is finished, you can see the Shards column is become to 1 from 0 Goto Nodes tab, we can add more readonly node for this shard Named the new node The readonly replica node has been created Now the new node is up, we add more replica nodes if neededNote: As the current instance is setup without the Cluster Mode is disabled, so we can only have one shard. More info Test ReplicationWrite Lambda Function Add new Lambda function to test the read write function: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465const redisReadOptions = &#123; host: "firstreplicanode.cbmmpl.0001.use1.cache.amazonaws.com", port: 6379&#125;const redisWriteOptions = &#123; host: "redistest.cbmmpl.0001.use1.cache.amazonaws.com", port: 6379&#125;...function getReadClient()&#123; console.info('Start to connect to Redis Server') var client = redis.createClient(redisReadOptions); console.info('Connected to Redis Server') return client&#125;function getWriteClient()&#123; console.info('Start to connect to Redis Server') var client = redis.createClient(redisWriteOptions); console.info('Connected to Redis Server') return client&#125;...case "GET": var client = getReadClient(); console.info('get all') client.hgetallAsync(GLOBAL_KEY).then(res =&gt; &#123; console.info('Redis responses for get all: ', res) callback(null, &#123;body: "This is a LIST operation, return all products", ret: res&#125;); // callback(null, &#123;body: "This is a LIST operation, return all products"&#125;); &#125;).catch(err =&gt; &#123; console.error("Failed to post data: ", err) callback(null, &#123;statusCode: 500, message: "Failed to get data"&#125;); &#125;).finally(() =&gt; &#123; console.info('Disconnect to Redis'); client.quit(); &#125;); break;case "POST": var client = getWriteClient(); if (data) &#123; console.info('Posting data for [', id, '] with value: ', data); client.hmsetAsync(GLOBAL_KEY, id, data).then(res =&gt; &#123; console.info('Redis responses for post: ', res) callback(null, &#123;body: "This is a CREATE operation and it's successful", ret: res&#125;); // callback(null, &#123;body: "This is a CREATE operation"&#125;); &#125;).catch(err =&gt; &#123; console.error("Failed to post data: ", err) callback(null, &#123;statusCode: 500, message: "Failed to post data"&#125;); &#125;).finally(() =&gt; &#123; console.info('Disconnect to Redis'); client.quit(); &#125;); &#125; else &#123; callback(null, &#123;statusCode: 500, message: 'no data is posted'&#125;) &#125; break; ... Then deploy the code Test Lambda Function Check the data in readonly node Write data to the read/write Primary node The re-check the data in readonly nodeWORKING!!! SummaryNo mater EC2 instance or Lambda function to access ElastiCache service, they must: Be in one VPC Contain same Subnet Contain same Security Group]]></content>
      <tags>
        <tag>AWS</tag>
        <tag>Redis</tag>
        <tag>ElastiCache</tag>
        <tag>Lambda</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Amazon ElastiCache for Redis]]></title>
    <url>%2F2018%2F03%2F29%2Faws-elasti-cache-redis%2F</url>
    <content type="text"><![CDATA[Amazon ElastiCache is a web service that makes it easy to set up, manage, and scale a distributed in-memory data store or cache environment in the cloud. It provides a high-performance, scalable, and cost-effective caching solution, while removing the complexity associated with deploying and managing a distributed cache environment. Note: Your Amazon ElastiCache instances can only be accessed through an Amazon EC2 instance. Sample IntroduceAmazon ElastiCache for Redis support for partitioning your data across up to 15 shards (called node groups in the ElastiCache API and AWS CLI).With Amazon ElastiCache for Redis, you can start small and easily scale your Redis data as your application grows - all the way up to a cluster with 6.1 TiB of in-memory data. It supports online cluster resizing to scale-out and scale-in your Redis clusters without downtime and adapts to changing demand. How it works Useful Case: How to build a caching application with ElastiCache for Redis Amazon ElastiCache for Redis offers List data structure making it easy to implement a lightweight, persistent queue. Amazon ElastiCache for Redis is highly suited as a session store to manage session information such as user authentication tokens, session state, and more. Simply use ElastiCache for Redis as a fast key-value store with appropriate TTL on session keys to manage your session information Use Amazon ElastiCache for RedisDetermine System Requirements Memory and ProcessorThe basic building block of Amazon ElastiCache is the node. Nodes are configured singularly or in groupings to form clusters. When determining the node type to use for your cluster, take the cluster’s node configuration and the amount of data you have to store into consideration.The Redis engine is single threaded, so a node’s number of cores is irrelevant. Cluster Configurations (More details)Amazon ElastiCache for Redis clusters provide a variety of node configurations: Single Node Single Shard Multiple NodesA single shard configuration groups up to 6 nodes in a single cluster, a read/write Primary node and up to 5 read-only Replica nodes. Multiple Shards Multiple NodesA multiple shard configuration partitions your data across up to 20 shards in the cluster. Each shard has a read/write Primary node and up to 5 read-only replica nodes. Scaling RequirementsRedis multiple shard clusters can be scaled out or in. To scale a Redis multiple shard cluster, you add or remove shards from the cluster. If your application is connecting to the cluster’s configuration endpoint, you do not need to make any changes in your application when you add or remove shards. Failover RequirementsWhen Automatic Failover is enabled, if a Redis Primary node fails for any reason, one of the shard’s read-only replica nodes is promoted to Primary. If Automatic Failover is not enabled and the Primary fails, ElastiCache for Redis spins up a new node to fill the Primary role. This operation is much more time consuming than failing over to a replica node.Automatic failover is only available on clusters running the Redis engine with multiple nodes. Access RequirementsAmazon ElastiCache are accessed from Amazon EC2 instances. Network access to an ElastiCache cluster is limited to the user account that created the cluster. Therefore, before you can access a cluster from an Amazon EC2 instance, you must authorize the Amazon EC2 instance to access the cluster. The steps to do this vary, depending upon whether you launched into EC2-VPC or EC2-Classic. Region and Availability Zone RequirementsAmazon ElastiCache supports all AWS regions. By locating your ElastiCache clusters in a region close to your application you can reduce latency. If your cluster has multiple nodes, locating your nodes in different Availability Zones can reduce the impact of failures on your cluster. Setup A ClusterCreate UserLogin AWS IMA, and create a user with AmazonElastiCacheFullAccess policy, which comes pre-provisioned with permission that the service requires to create a Service Linked Role on your behalf.If you are not using the default policy and choose to use a custom managed policy, ensure you have either permissions to call iam:createServiceLinkedRole or you have created the ElastiCache Service Linked Role. Launch a Cluster To create a standalone Redis (cluster mode disabled) cluster: Sign in to the AWS Management Console Choose Get Started Now. Choose the region Choose Redis as cluster engine Complete ‘Redis settings’ Complete ‘Advanced Redis settings’ The other items are kept as default Choose Create cluster to launch your cluster Note: These configurations are used for test, there will be a couple of difference for Prod.Refer:https://docs.aws.amazon.com/zh_cn/AmazonElastiCache/latest/UserGuide/GettingStarted.CreateCluster.htmlhttps://docs.aws.amazon.com/AmazonElastiCache/latest/UserGuide/Replication.CreatingReplGroup.NoExistingCluster.Cluster.html View Cluster DetailsThe ‘create’ action will take a while. To view a Redis (cluster mode disabled) cluster’s details Login to Amazon ElastiCache console View the details of cluster cluster we created Click on the cluster’s name, the node(s) in the cluster will be listed, and you can see the metrics of each node Authorize Access AS Amazon ElastiCache instance can only be accessed through an Amazon EC2 instance, all ElastiCache clusters are designed to be accessed from an Amazon EC2 instance. The most common scenario is to access an ElastiCache cluster from an Amazon EC2 instance in the same Amazon Virtual Private Cloud (Amazon VPC). Now the ElastiCache instance should be launched into EC2-VPC. You can connect to your ElastiCache cluster only from an Amazon EC2 instance that is running in the same Amazon VPC. In this case, you will need to grant network ingress to the cluster. To grant network ingress from an Amazon VPC security group to a cluster: Login Amazon EC2 console Configure Security Groups Edit or Add Security GroupsAWS recommend creating a VPC Security Group that will be used exclusively by ElastiCache. Attach the Security Groups to the EC2 instance: Connect to a Cluster’s Node Find Primary Endpoint:Get the Primary Endpoint from cluster’s details Also we can get the cluster’s details by AWS CLI: 123aws elasticache describe-cache-clusters \ --cache-cluster-id elasticachefirst \ --show-cache-node-info 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647&#123; "CacheClusters": [ &#123; "CacheClusterId": "elasticachefirst", "ClientDownloadLandingPage": "https://console.aws.amazon.com/elasticache/home#client-download:", "CacheNodeType": "cache.t2.micro", "Engine": "redis", "EngineVersion": "3.2.10", "CacheClusterStatus": "available", "NumCacheNodes": 1, "PreferredAvailabilityZone": "cn-north-1a", "CacheClusterCreateTime": "2018-03-29T07:48:39.519Z", "PreferredMaintenanceWindow": "fri:20:00-fri:21:00", "PendingModifiedValues": &#123;&#125;, "CacheSecurityGroups": [], "CacheParameterGroup": &#123; "CacheParameterGroupName": "default.redis3.2", "ParameterApplyStatus": "in-sync", "CacheNodeIdsToReboot": [] &#125;, "CacheSubnetGroupName": "elasticachetestnet", "CacheNodes": [ &#123; "CacheNodeId": "0001", "CacheNodeStatus": "available", "CacheNodeCreateTime": "2018-03-29T07:48:39.519Z", "Endpoint": &#123; "Address": "elasticachefirst.xyz.0001.abc.cache.amazonaws.com.cn", "Port": 6379 &#125;, "ParameterGroupStatus": "in-sync", "CustomerAvailabilityZone": "cn-north-1a" &#125; ], "AutoMinorVersionUpgrade": true, "SecurityGroups": [ &#123; "SecurityGroupId": "sg-031e8267", "Status": "active" &#125; ], "AuthTokenEnabled": false, "TransitEncryptionEnabled": false, "AtRestEncryptionEnabled": false &#125; ]&#125; A Redis (cluster mode disabled) primary endpoint looks something like the above. Connect to a Redis Cluster or Replication GroupNow that you have the endpoint you need, you can log in to an EC2 instance and connect to the cluster or replication group. Login to EC2 instance Use the redis-cli utility to connect to a cluster that is not encryption enabled and running RedisMaybe you need to install redis-cli: 1234567sudo yum install gccwget http://download.redis.io/redis-stable.tar.gztar xvzf redis-stable.tar.gzcd redis-stablemake distclean // Ubuntu systems onlymakecp src/redis-cli /usr/bin/ Check If Redis is working: 1234$ redis-cli -h elasticachefirst.xxxx.0001.cnn1.cache.amazonaws.com.cnelasticachefirst.xxxx.0001.cnn1.cache.amazonaws.com.cn:6379&gt; pingPONGelasticachefirst.xxxx.0001.cnn1.cache.amazonaws.com.cn:6379&gt; The Redis Server is working! Note: The ElastiCache instance and EC2 instance must have same Security Group, otherwise EC2 instance cannot connect to ElastiCache. Now the Redis Server on AWS is up. For Prod, we should set Redis as Multiple Shards Multiple Nodes. Reference https://docs.aws.amazon.com/AmazonElastiCache/latest/UserGuide/GettingStarted.AuthorizeAccess.html]]></content>
      <tags>
        <tag>AWS</tag>
        <tag>Redis</tag>
        <tag>ElastiCache</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DynamoDB 待续]]></title>
    <url>%2F2018%2F03%2F28%2Faws-dynamodb-4-incomplete%2F</url>
    <content type="text"><![CDATA[待完善及确认的地方: 全局二级索引 &amp; 本地二级索引 区别生存时间自动删除过期的项，从而帮助您降低存储用量，减少用于存储不相关数据的成本 吞吐量: 消耗, 设置, 优化读取和写入吞吐容量 在 Amazon DynamoDB 中创建表或索引时，必须指定读写活动的容量要求。通过提前定义您的吞吐容量，DynamoDB 可以预留必要的资源，以满足您的应用程序所需的读写活动，同时确保一致的低延迟性能。 一个 读取容量单位 表示对大小最多为 4 KB 的项目每秒执行一次强一致性读取，或每秒执行两次最终一致性读取。如果您需要读取大于 4 KB 的项目，DynamoDB 需要消耗额外的读取容量单位。所需的读取容量单位的总数取决于项目大小以及您需要最终一致性读取还是强一致性读取。 一个 写入容量单位 表示对大小最多为 1 KB 的项目每秒执行一次写入。如果您需要写入大于 1 KB 的项目，DynamoDB 需要消耗额外的写入容量单位。所需的写入容量单位的总数取决于项目大小。 例如，假设您创建了具有 5 个读取容量单位和 5 个写入容量单位的表。使用这些设置，您的应用程序可以： 1. 执行高达每秒 20KB 的强一致性读取 (4 KB × 5 个读取容量单位)。 2. 执行高达每秒 40KB 的最终一致性读取 (读取吞吐量的两倍)。 3. 每秒写入高达 5KB (1 KB × 5 个写入容量单位)。 如果您的应用程序读取或写入较大型的项目 (最大为 DynamoDB 的项目大小上限 400 KB)，它将消耗更多的容量单位。 您的读取或写入请求超过了表的吞吐量设置，则 DynamoDB 可能会限制 该请求。DynamoDB 也可以限制超过了索引吞吐量的读取请求。限制会阻止您的应用程序消耗太多容量单位。当请求受到限制时，它将失败并出现代码 HTTP 400 (Bad Request) 和一个 ProvisionedThroughputExceededException 1. 增加预置吞吐量 可以根据需要使用 AWS 管理控制台或 UpdateTable 操作增加 ReadCapacityUnits 或 WriteCapacityUnits。 2. 减少预置吞吐量 每天最多可执行 4 次调低操作. 如果过去 4 小时内未执行减小操作，则可以执行额外的调低操作 备份利用 DAX 实现内存中加速监控 DynamoDB与其他 AWS 服务集成事务: 最终一致性DynamoDB Service]]></content>
      <tags>
        <tag>DynamoDB</tag>
        <tag>AWS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Dynamoose 使用]]></title>
    <url>%2F2018%2F03%2F28%2Faws-dynamodb-3-dynamoose%2F</url>
    <content type="text"><![CDATA[Dynamoose 是一个DynamoDB的建模工具. 安装1npm install dynamoose 使用配置 环境变量 1234# Set environment variablesexport AWS_ACCESS_KEY_ID=&quot;Your AWS Access Key ID&quot;export AWS_SECRET_ACCESS_KEY=&quot;Your AWS Secret Access Key&quot;export AWS_REGION=&quot;us-east-1&quot; 配置AWS Object 12345dynamoose.AWS.config.update(&#123; accessKeyId: 'AKID', secretAccessKey: 'SECRET', region: 'us-east-1'&#125;); AWS IAM role配置https://docs.aws.amazon.com/zh_cn/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html 本地 12dynamoose.local(); // This will set the server to &quot;http://localhost:8000&quot; (default)dynamoose.local(&quot;http://localhost:1234&quot;) // This will set the server to &quot;http://localhost:1234&quot; 建模及使用建模12345678910111213141516171819202122232425262728293031323334353637383940414243&#123; name: 'DMovie', schema: &#123; fields: &#123; year: &#123; type: Number, validate: function (v) &#123; return v &gt; 0; &#125;, hashKey: true &#125;, title: &#123; type: String, rangeKey: true, index: true, name: "title-index", ProjectionType: "ALL" &#125;, info: &#123; type: Object, &#125;, flag: &#123; type: Number, index: true, name: "flag-index", // ProjectionType: ALL &#125;, &#125;, config: &#123; throughput: &#123; // Sets the throughput of the DynamoDB table on creation read: 5, write: 2 &#125;, useNativeBooleans: true, // Store Boolean values as Boolean (‘BOOL’) in DynamoDB useDocumentTypes: true, //Store Objects and Arrays as Maps (‘M’) and Lists (‘L’) types in DynamoDB. saveUnknown: true, // Specifies that attributes not defined in the schema will be saved and retrieved. This defaults to false. timestamps: &#123; // Defines that schema must contain fields to control creation and last update timestamps. If it is set to true, this fields will be createdAt for creation date and updatedAt for last update createdAt: 'creationDate', updatedAt: 'lastUpdateDate' &#125; &#125; &#125;&#125; setting12345678&#123; create: true, // Create table in DB, if it does not exist, update: false, // Update remote indexes if they do not match local index structure waitForActive: true, // Wait for table to be created before trying to use it waitForActiveTimeout: 180000, // wait 3 minutes for table to activate prefix: &apos;&apos;, // Default prefix for all DynamoDB tables suffix: &apos;&apos; // Default suffix for all DynamoDB tables&#125; 1var DMovie = builder(config); 添加记录1234567891011return DMovie.create(&#123; "year": 2018, "title": "Rush 1", "info": "Info 1"&#125;).then(data =&gt; &#123; expect(data.year).to.be.equal(2018); expect(data.title).to.be.equal('Rush 1'); expect(data).to.be.have.property('creationDate'); expect(data).to.be.have.property('lastUpdateDate');&#125;); 查询12345678910111213it('DMovie record should be query successfully', () =&gt; &#123; return DMovie.get(&#123; "year": 2018, "title": "Rush 1" &#125;).then(data =&gt; &#123; console.log('data ', data) expect(data.year).to.be.equal(2018); expect(data.title).to.be.equal('Rush 1'); expect(data.info).to.be.equal('Info 1'); expect(data).to.be.have.property('creationDate'); expect(data).to.be.have.property('lastUpdateDate'); &#125;);&#125;); 更新记录123456789101112return DMovie.update(&#123; &quot;year&quot;: 2018, &quot;title&quot;: &quot;Rush 1&quot;,&#125;,&#123;&quot;test&quot;: &quot;I am updated&quot;, &quot;info&quot;: &quot;Info 2&quot;&#125;).then(data =&gt; &#123; expect(data.year).to.be.equal(2018); expect(data.title).to.be.equal(&apos;Rush 1&apos;); expect(data.info).to.be.equal(&apos;Info 2&apos;); expect(data).to.be.have.property(&apos;creationDate&apos;); expect(data).to.be.have.property(&apos;lastUpdateDate&apos;);&#125;); 删除记录12345678it(&apos;DMovie record should be deleted successfully&apos;, () =&gt; &#123; return DMovie.delete(&#123; &quot;year&quot;: 2018, &quot;title&quot;: &quot;Rush 1&quot; &#125;).then((data) =&gt; &#123; expect(data.year).to.be.equal(2018); &#125;);&#125;); 批量添加12345678910111213141516171819202122232425it(&apos;DMovie record should be batch added successfully&apos;, () =&gt; &#123; var allMovies = JSON.parse(fs.readFileSync(normalPath + &apos;/movie2.json&apos;, &apos;utf8&apos;)); expect(allMovies.length).to.be.equal(655); var times = 10, flag = 0, items = []; var iter = function (model) &#123; flag++; var v = flag % 3; items.push(new DMovie(&#123; &quot;year&quot;: 2013, &quot;title&quot;: &quot;v.&quot; + flag + &apos; &apos; + model.title, &quot;info&quot;: model.info, &quot;flag&quot;: v &#125;)); &#125;; for (var i = 0; i &lt; times; i++) &#123; allMovies.forEach(iter); &#125; return DMovie.batchPut(items).then(data =&gt; &#123; expect(Object.keys(data.Responses)).to.have.lengthOf(0); expect(Object.keys(data.UnprocessedItems)).to.have.lengthOf(0); &#125;);&#125;); DynamoDB 低级 API 支持批量读取和写入操作.批量操作可以容忍批处理中的个别请求失败。举例来说，假设一个 BatchGetItem 请求读取五个项目。即使某些底层 GetItem 请求失败，这也不会导致整个 BatchGetItem 操作失败。另一方面，如果所有五个读取操作都失败，则整个 BatchGetItem 将失败。批量操作会返回有关各失败请求的信息，以便您诊断问题并重试操作。对于 BatchGetItem，在请求的 UnprocessedKeys 参数中会返回有问题的表和主键。对于 BatchWriteItem，在 UnprocessedItems 中返回类似信息。如果 DynamoDB 返回了任何未处理的项目，应对这些项目重试批量操作。然而，我们强烈建议您使用指数回退算法。如果立即重试批量操作，底层读取或写入请求仍然会由于各表的限制而失败。如果使用指数回退延迟批量操作，批处理中的各请求成功的可能性更大。 查询索引12345678910it(&apos;DMovie record with index#title-index should be counted successfully&apos;, () =&gt; &#123; // same as: DMovie.query(&apos;flag&apos;).using(&apos;FlagTitleIndex&apos;).eq(0).exec().then(result =&gt; &#123; return DMovie.query(&apos;flag&apos;).using(&apos;FlagTitleIndex&apos;).eq(0).exec().then(result =&gt; &#123; console.log(&apos;result&apos;, result) // result.splice(0,result.length) expect(result.count).to.be.equal(1824); expect(result.scannedCount).to.be.equal(1824); expect(Object.keys(result.lastKey)).to.have.lengthOf(3); &#125;);&#125;); 删除表123456789it(&apos;deleteTable should be called successfully&apos;, () =&gt; &#123; console.log(&apos;DMovie.tableName &apos;, DMovie.modelName) return deleteTable(&#123; TableName: DMovie.modelName &#125;).then((result) =&gt; &#123; expect(result.TableDescription).to.have.property(&apos;CreationDateTime&apos;); expect(result.TableDescription).to.have.property(&apos;TableArn&apos;); &#125;);&#125;);]]></content>
      <tags>
        <tag>DynamoDB</tag>
        <tag>AWS</tag>
        <tag>Dynamoose</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DynamoDB 开发 - JS]]></title>
    <url>%2F2018%2F03%2F26%2Faws-dynamodb-2%2F</url>
    <content type="text"><![CDATA[DynamoDB in ‘node.js’DynamoDB Local本地运行DynamoDB作为开发环境, 下载后本地解压运行:1java -Djava.library.path=./DynamoDBLocal_lib -jar DynamoDBLocal.jar -sharedDb aws-sdk依赖于aws-sdk:1npm install --save aws-sdk 注意: DynamoDB Local和 DynamoDB Web 服务是有差异的: https://docs.aws.amazon.com/zh_cn/amazondynamodb/latest/developerguide/DynamoDBLocal.UsageNotes.html Dev查看已存在的表123456789var DynamoDb = new AWS.DynamoDB();DynamoDb.listTables(function (err, data) &#123; if (err) &#123; console.error("Unable to list tables. Error JSON:", JSON.stringify(err, null, 2)); &#125; else &#123; console.log(data); &#125;&#125;); 创建表12345678910111213141516171819202122232425var DynamoDb = new AWS.DynamoDB();var tableSchema = &#123; TableName: "Movies" KeySchema: [ &#123;AttributeName: "year", KeyType: "HASH"&#125;, //Partition key &#123;AttributeName: "title", KeyType: "RANGE"&#125; //Sort key ], AttributeDefinitions: [ &#123;AttributeName: "year", AttributeType: "N"&#125;, &#123;AttributeName: "title", AttributeType: "S"&#125; ], ProvisionedThroughput: &#123; ReadCapacityUnits: 10, WriteCapacityUnits: 10 &#125;&#125;DynamoDb.createTable(tableSchema, function (err, data) &#123; if (err) &#123; console.error("Unable to create table. Error JSON:", JSON.stringify(err, null, 2)); &#125; else &#123; console.log(data); &#125;&#125;); 注意: KeySchema - 必须有且仅有一个分区键, 排序键可以没有, 但最多只有一个.表的大小没有实际限制。表的项目数和字节数是无限制的对于任何 AWS 账户，每个区域的初始限制为 256 个表 全局表全局表 为部署多区域、多主机数据库提供了完全托管的解决方案，而不必构建和维护您自己的复制解决方案。在创建全局表时，指定要在其中提供表的 AWS 区域。DynamoDB 执行在这些区域中创建相同的表并将持续数据更改传播到所有这些表所必需的所有任务。 查看表的详情12345678var DynamoDb = new AWS.DynamoDB();DynamoDb.describeTable(table, function (err, data) &#123; if (err) &#123; console.error("Unable to describe table#", table.TableName, ". Error JSON:", JSON.stringify(err, null, 2)); &#125; else &#123; console.log(data); &#125;&#125;); 一般输出如下:12345678910111213141516171819202122232425262728293031323334353637&#123; "Table": &#123; "AttributeDefinitions": [ &#123; "AttributeName": "year", "AttributeType": "N" &#125;, &#123; "AttributeName": "title", "AttributeType": "S" &#125; ], "TableName": "Movies", "KeySchema": [ &#123; "AttributeName": "year", "KeyType": "HASH" &#125;, &#123; "AttributeName": "title", "KeyType": "RANGE" &#125; ], "TableStatus": "ACTIVE", "CreationDateTime": "2018-03-28T02:49:45.326Z", "ProvisionedThroughput": &#123; "LastIncreaseDateTime": "1970-01-01T00:00:00.000Z", "LastDecreaseDateTime": "1970-01-01T00:00:00.000Z", "NumberOfDecreasesToday": 0, "ReadCapacityUnits": 10, "WriteCapacityUnits": 10 &#125;, "TableSizeBytes": 28, "ItemCount": 1, "TableArn": "arn:aws:dynamodb:ddblocal:000000000000:table/Movies" &#125;&#125; 添加项123456789101112131415161718var DynamoDBDocClient = new AWS.DynamoDB.DocumentClient();var model = &#123; TableName: "Movies" Item: &#123; "year": 2018, "title": "Rush 1", "info": "Info 1" &#125;&#125;;DynamoDBDocClient.put(record, function (err, data) &#123; if (err) &#123; console.error("Unable to add ", record.TableName, ". Error JSON:", JSON.stringify(err, null, 2)); &#125; else &#123; console.log(data); &#125;&#125;); 注意: DocumentClient.put方法无返回, 除非出错了 查询和扫描Query 操作基于主键值查找项目.Scan 操作读取表或二级索引中的每个项目 (全部的). 1234567891011121314151617181920var DynamoDBDocClient = new AWS.DynamoDB.DocumentClient();var model = &#123; TableName: "Movies" KeyConditionExpression: "#yr = :yyyy", ExpressionAttributeNames: &#123; "#yr": "year" &#125;, ExpressionAttributeValues: &#123; ":yyyy": 2018 &#125;&#125;;DynamoDBDocClient.query(params, function (err, data) &#123; if (err) &#123; console.error("Failed to search ", tableName, ". Error JSON:", JSON.stringify(err, null, 2)); &#125; else &#123; console.log('data: ', data) &#125;&#125;); 返回如下12345&#123; Items: [&#123;title: 'Rush 1', year: 2018, info: 'Info 1'&#125;], Count: 1, ScannedCount: 1&#125; Query graph BT subgraph AWS DynamoDB D1(Apply KeyConditionExpression) D2(Apply Page Size - Limit) D3(Apply 1M Limitation) D4(Apply FilterExpression) D1 --> D2 D2 --> D3 D3 --> D4 end A[Query/Scan] --> |request|D1 D4 --> |Response|B[Result Pagination: LastEvaluatedKey & ExclusiveStartKey] B -. Next Page .- A 更新表1234567891011121314151617181920212223242526272829303132333435363738394041var DynamoDb = new AWS.DynamoDB();var tableSchema = &#123; TableName: "Movies" AttributeDefinitions: [ &#123;"AttributeName": "flag", "AttributeType": "N"&#125;, &#123;"AttributeName": "title", "AttributeType": "S"&#125; ], GlobalSecondaryIndexUpdates: [ &#123; "Create": &#123; "IndexName": "flag_title", "KeySchema": [ &#123; "AttributeName": "flag", "KeyType": "HASH" &#125;, &#123; "AttributeName": "title", "KeyType": "RANGE" &#125; ], "Projection": &#123; "NonKeyAttributes": ["info"], "ProjectionType": "INCLUDE" &#125;, "ProvisionedThroughput": &#123; "ReadCapacityUnits": 1, "WriteCapacityUnits": 1 &#125; &#125; &#125; ]&#125;DynamoDb.updateTable(tableSchema, function (err, data) &#123; if (err) &#123; console.error("Unable to update table. Error JSON:", JSON.stringify(err, null, 2)); &#125; else &#123; console.log(data); &#125;&#125;); 更新项123456789101112131415161718192021222324252627282930var DynamoDBDocClient = new AWS.DynamoDB.DocumentClient();var record = &#123; TableName: "Movies" Key: &#123; "year": 2018, "title": 'Wangy Test' &#125;, ConditionExpression: "#i = :iv", ExpressionAttributeNames: &#123; "#f": "flag", "#s": "size", "#i": "info" &#125;, ExpressionAttributeValues: &#123; ":fv": 1, ":sv": 150, ":iv": 'Wangy Test' &#125;, UpdateExpression: "set #f = :fv, #s = :sv", ReturnValues:"UPDATED_NEW"&#125;;DynamoDBDocClient.update(record, function (err, data) &#123; if (err) &#123; console.error("Unable to update ", record.TableName, ". Error JSON:", JSON.stringify(err, null, 2)); &#125; else &#123; console.log(data); &#125;&#125;); 可以操作项: SET - 修改或添加项目属性 REMOVE - 从项目中删除属性 ADD - 更新数字和集合 DELETE - 从集合中删除元素 参考: https://docs.aws.amazon.com/zh_cn/amazondynamodb/latest/APIReference/API_UpdateItem.html#API_UpdateItem_RequestSyntax https://docs.aws.amazon.com/zh_cn/amazondynamodb/latest/developerguide/Expressions.UpdateExpressions.html 删除项1234567891011121314151617181920212223var DynamoDBDocClient = new AWS.DynamoDB.DocumentClient();var record = &#123; TableName: "Movies" Key: &#123; "year": 2018, "title": 'Wangy Test' &#125;, ConditionExpression: "#yr = :yyyy", ExpressionAttributeNames: &#123; "#yr": "year" &#125;, ExpressionAttributeValues: &#123; ":yyyy": 2018 &#125;&#125;;DynamoDBDocClient.delete(record, function (err, data) &#123; if (err) &#123; console.error("Unable to delete ", record.TableName, ". Error JSON:", JSON.stringify(err, null, 2)); &#125; else &#123; console.log(data); &#125;&#125;); 删除表1234567891011var DynamoDBDocClient = new AWS.DynamoDB.DocumentClient();var table = &#123; TableName: "Movies"&#125;;DynamoDb.deleteTable(table, function (err, data) &#123; if (err) &#123; console.error("Unable to drop table. Error JSON:", JSON.stringify(err, null, 2)); &#125; else &#123; console.log('table desc: ', data) &#125;&#125;); 延伸数据类型以下是 DynamoDB 数据类型描述符的完整列表： S – 字符串 N – 数字 B – 二进制 BOOL – 布尔值 NULL – Null M – 映射 L – 列表 SS – 字符串集 NS – 数字集 BS – 二进制集 属性分类： 标量类型 - 标量类型可准确地表示一个值。标量类型包括数字、字符串、二进制、布尔值和 null。 S – 字符串 N – 数字数字可为正数、负数或零。数字最多可精确到 38 位。超过此位数将导致异常。 正数范围：1E-130 到 9.9999999999999999999999999999999999999E+125 负数范围：-9.9999999999999999999999999999999999999E+125 到 -1E-130数字以可变长度形式表示。系统会删减开头和结尾的 0. B – 二进制 二进制属性的长度必须大于零且受限于最大 DynamoDB 项目大小 400 KB 将主键属性定义为二进制类型属性，以下附加限制将适用： 对于简单的主键，第一个属性值 (分区键) 的最大长度为 2048 字节。 对于复合主键，第二个属性值 (排序键) 的最大长度为 1024 字节。 在将二进制值发送到 DynamoDB 之前，您的应用程序必须采用 Base64 编码格式对其进行编码。收到这些值后，DynamoDB 会将数据解码为无符号字节数组，将其用作二进制属性的长度 BOOL – 布尔值 NULL – Null空代表属性具有未知或未定义状态。 文档类型 - 文档类型可表示具有嵌套属性的复杂结构 - 例如您将在 JSON 文档中找到的结构。文档类型包括列表和映射。文档类型包括列表和映射。这些数据类型可以互相嵌套，用来表示深度最多为 32 层的复杂数据结构。只要包含值的项目大小在 DynamoDB 项目大小限制 (400 KB) 内，列表或映射中值的数量就没有限制。属性值不能是空字符串或空集 (字符串集、数字集或二进制集)，但可以是空列表和映射。 M – 映射映射类型属性可以存储名称/值对的无序集合。映射用大括号括起：{ … }映射类似于 JSON 对象。映射元素中可以存储的数据类型没有限制，映射中的元素也不一定为相同类型 L – 列表列表类型属性可存储值的有序集合。列表用方括号括起：[ … ]列表类似于 JSON 数组。列表元素中可以存储的数据类型没有限制，列表元素中的元素也不一定为相同类型。 集类型 - 集类型可表示多个标量值。集类型包括字符串集、数字集和二进制集。 主键属性定义为字符串类型属性，以下附加限制将适用： 对于简单的主键，第一个属性值 (分区键) 的最大长度为 2048 字节。 对于复合主键，第二个属性值 (排序键) 的最大长度为 1024 字节。 日期DynamoDB没有日期类型, 解决方案: 使用字符串数据类型表示日期或时间戳。执行此操作的一种方法是使用 ISO 8601 字符串，如以下示例所示： 2016-02-15 2015-12-21T17:42:34Z 20150311T122706Z有关更多信息，请访问 http://en.wikipedia.org/wiki/ISO_8601 使用数字数据类型表示日期或时间戳。执行此操作的一种方法是使用纪元时间 - 自 1970 年 1 月 1 日 00:00:00 UTC 以来的秒数。例如，纪元时间 1437136300 表示 2015 年 7 月 17 日 12:31:40 UTC。有关更多信息，请访问 http://en.wikipedia.org/wiki/Unix_time。 命名个规则 DynamoDB 中的表、属性和其他对象必须具有名称.所有名称都必须使用 UTF-8 进行编码，并且区分大小写。表名称和索引名称的长度必须介于 3 到 255 个字符之间，而且只能包含以下字符： a-z A-Z 0-9 _ (下划线) (短划线) . (圆点) 属性名称的长度必须介于 1 到 255 个字符之间。具有特殊含义：# (散列) 和 : (冒号)保留关键字: https://docs.aws.amazon.com/zh_cn/amazondynamodb/latest/developerguide/ReservedWords.html 读取一致性 Amazon DynamoDB 在全世界多个 AWS 区域可用。每个区域均与其他 AWS 区域独立和隔离 每个 AWS 区域包含多个不同的称为“可用区”的位置。每个可用区都与其他可用区中的故障隔离，并提供与同一区域其他可用区的低成本、低延迟网络连接。这使您可以在某个区域的多个可用区之间快速复制数据。 当您的应用程序将某个数据写入 DynamoDB 表并收到 HTTP 200 响应 (OK) 时，该数据的所有副本都会更新。该数据最终将在所有存储位置中保持一致，通常只需一秒或更短时间。 最终一致性读取当您从 DynamoDB 表中读取数据时，响应反映的可能不是刚刚完成的写入操作的结果。响应可能包含某些陈旧数据。如果您在短时间后重复读取请求，响应将返回最新的数据。 强一致性读取当您请求强一致性读取时，DynamoDB 会返回具有最新数据的响应，从而反映来自所有已成功的之前写入操作的更新。如果网络延迟或中断，可能会无法执行强一致性读取。读取操作 (例如 GetItem，Query 和 Scan) 提供了一个 ConsistentRead 参数。如果您将此参数设置为 true，DynamoDB 将在操作过程中使用强一致性读取。 Count问题https://stackoverflow.com/questions/12499822/how-can-i-get-the-total-number-of-items-in-a-dynamodb-table]]></content>
      <tags>
        <tag>DynamoDB</tag>
        <tag>AWS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[AWS DynamoDB 学习]]></title>
    <url>%2F2018%2F03%2F21%2Faws-dynamodb-1%2F</url>
    <content type="text"><![CDATA[DynamoDB 的按需备份和还https://docs.aws.amazon.com/zh_cn/amazondynamodb/latest/developerguide/BackupRestore.html 生存时间https://docs.aws.amazon.com/zh_cn/amazondynamodb/latest/developerguide/TTL.html自动删除过期的项，从而帮助您降低存储用量，减少用于存储不相关数据的成本 索引 在创建索引时应谨慎, 只要对表进行写入，就必须更新表的所有索引。在具有大型表的写入密集型环境中，这会占用大量系统资源。DynamoDB 中的索引与其关系对应项不同。当您创建二级索引时，必须指定其键属性 - 分区键和排序键。在创建二级索引后，您可以对它执行 Query 或 Scan 操作，就如同对表执行这些操作一样。DynamoDB 没有查询优化程序，因此，仅在您对二级索引执行 Query 或 Scan 操作时使用它。索引的键架构。索引键架构中的每个属性必须是类型为字符串、数字或二进制的顶级属性。其他数据类型，包括文档和集，均不受支持。键架构的其他要求取决于索引的类型： 对于全局二级索引，分区键可以是基表的任何标量属性。排序键是可选的，也可以是基表的任何标量属性。 对于local secondary index，分区键必须与基表的分区键相同，排序键必须是非键基表属性。 DynamoDB 支持两种不同的索引： 全局二级索引 - 索引的主键可以是其表中的任意两个属性。 本地二级索引 - 索引的分区键必须与其表的分区键相同。不过，排序键可以是任何其他属性。 您最多可以为每个表定义 5 个local secondary index和 5 个全局二级索引。AWS 账户需要支付在基表中存储项目以及在表的任何全局二级索引中存储属性的费用. 索引项目所占用的空间大小就是以下内容之和： 基表的主键 (分区键和排序键) 的大小 (按字节计算) 索引键属性的大小（按字节计算） 投影的属性（如果有）的大小（按字节计算） 每个索引项目 100 bytes 的开销要估算全局二级索引的存储要求，您可以估算索引中项目的平均大小，然后乘以基表中具有全局二级索引键属性的项目数 本地二级索引: 主键必须是复合主键（分区键和排序键） 分区键必须与基表的分区键相同，排序键必须是非键基表属性, 排序键仅由一个标量属性构成 每个表创建最多 5 个local secondary index 查询或扫描local secondary index，可以请求未投影到索引中的属性 查询local secondary index时，您可以选择最终一致性或强一致性 在创建表的时候建立, 后期无法增删(Update操作不能更新本地二级索引) 对于每个分区键值，所有索引+项目的大小总和必须为 10GB 或更小, 参考 项目集合意思是: 表中具有同一个分区键所有项目的存储容量, 再加上本地二级索引的存储容量(基表的主键 + 索引键属性的大小 + 投影的属性大小 + N * 每个索引项目 100)之和不得超过10G.如果项目集合的大小超出 10GB 的限制，则 DynamoDB 返回 ItemCollectionSizeLimitExceededException，并且您无法向该项目集合添加更多项目，也不能增加该项目集合中项目的大小。（仍允许执行可减小项目集合大小的读写操作。）您仍然可以向其他项目集合中添加项目。 全局二级索引: 分区键可以是基表的任何标量属性。排序键是可选的，也可以是基表的任何标量属性. 每个索引键的属性必须是标量类型：String、Number 或 Binary。 (不可以是文档或集。)您可以将任何数据类型的属性投影到全局二级索引中，其中包括标量、文档和集 对于全局二级索引查询或扫描，您只能请求投影到索引中的属性 每个表创建最多 5 个全局二级索引 查询仅支持最终一致性 可以通过Update操作进行增删 在表中放置或删除项目时，表的全局二级索引会以最终一致性方式进行更新。在正常情况下，对表数据进行的更改会瞬间传播到全局二级索引。但是，在某些不常发生的故障情况下，可能出现较长时间的传播延迟。因此，应用程序需要预计和处理对全局二级索引进行的查询返回不是最新结果的情况。 必须为索引提供 ProvisionedThroughput 设置，该设置包括 ReadCapacityUnits 和 WriteCapacityUnits。这些预置吞吐量设置独立于表的相应设置，但是行为是类似的. 一般原则是，建议将索引的预置写入容量设置为表的写入容量的 1.5 倍。 在索引处于 ACTIVE 状态之前，您无法对其执行 Query 或 Scan 操作。 数据类型以下是 DynamoDB 数据类型描述符的完整列表： S – 字符串 N – 数字 B – 二进制 BOOL – 布尔值 NULL – Null M – 映射 L – 列表 SS – 字符串集 NS – 数字集 BS – 二进制集 属性分类： 标量类型 - 标量类型可准确地表示一个值。标量类型包括数字、字符串、二进制、布尔值和 null。 S – 字符串 N – 数字数字可为正数、负数或零。数字最多可精确到 38 位。超过此位数将导致异常。 正数范围：1E-130 到 9.9999999999999999999999999999999999999E+125 负数范围：-9.9999999999999999999999999999999999999E+125 到 -1E-130数字以可变长度形式表示。系统会删减开头和结尾的 0. B – 二进制 二进制属性的长度必须大于零且受限于最大 DynamoDB 项目大小 400 KB 将主键属性定义为二进制类型属性，以下附加限制将适用： 对于简单的主键，第一个属性值 (分区键) 的最大长度为 2048 字节。 对于复合主键，第二个属性值 (排序键) 的最大长度为 1024 字节。 在将二进制值发送到 DynamoDB 之前，您的应用程序必须采用 Base64 编码格式对其进行编码。收到这些值后，DynamoDB 会将数据解码为无符号字节数组，将其用作二进制属性的长度 BOOL – 布尔值 NULL – Null空代表属性具有未知或未定义状态。 文档类型 - 文档类型可表示具有嵌套属性的复杂结构 - 例如您将在 JSON 文档中找到的结构。文档类型包括列表和映射。文档类型包括列表和映射。这些数据类型可以互相嵌套，用来表示深度最多为 32 层的复杂数据结构。只要包含值的项目大小在 DynamoDB 项目大小限制 (400 KB) 内，列表或映射中值的数量就没有限制。属性值不能是空字符串或空集 (字符串集、数字集或二进制集)，但可以是空列表和映射。 M – 映射映射类型属性可以存储名称/值对的无序集合。映射用大括号括起：{ … }映射类似于 JSON 对象。映射元素中可以存储的数据类型没有限制，映射中的元素也不一定为相同类型 L – 列表列表类型属性可存储值的有序集合。列表用方括号括起：[ … ]列表类似于 JSON 数组。列表元素中可以存储的数据类型没有限制，列表元素中的元素也不一定为相同类型。 集类型 - 集类型可表示多个标量值。集类型包括字符串集、数字集和二进制集。 主键属性定义为字符串类型属性，以下附加限制将适用： 对于简单的主键，第一个属性值 (分区键) 的最大长度为 2048 字节。 对于复合主键，第二个属性值 (排序键) 的最大长度为 1024 字节。 日期 使用字符串数据类型表示日期或时间戳。执行此操作的一种方法是使用 ISO 8601 字符串，如以下示例所示： 2016-02-15 2015-12-21T17:42:34Z 20150311T122706Z有关更多信息，请访问 http://en.wikipedia.org/wiki/ISO_8601 使用数字数据类型表示日期或时间戳。执行此操作的一种方法是使用纪元时间 - 自 1970 年 1 月 1 日 00:00:00 UTC 以来的秒数。例如，纪元时间 1437136300 表示 2015 年 7 月 17 日 12:31:40 UTC。有关更多信息，请访问 http://en.wikipedia.org/wiki/Unix_time。 DynamoDB 中的表、属性和其他对象必须具有名称所有名称都必须使用 UTF-8 进行编码，并且区分大小写。 表名称和索引名称的长度必须介于 3 到 255 个字符之间，而且只能包含以下字符： 1. a-z 2. A-Z 3. 0-9 4. _ (下划线) 5. - (短划线) 6. . (圆点) 属性名称的长度必须介于 1 到 255 个字符之间。具有特殊含义：# (散列) 和 : (冒号)保留关键字: https://docs.aws.amazon.com/zh_cn/amazondynamodb/latest/developerguide/ReservedWords.html 读取一致性1. Amazon DynamoDB 在全世界多个 AWS 区域可用。每个区域均与其他 AWS 区域独立和隔离 2. 每个 AWS 区域包含多个不同的称为“可用区”的位置。每个可用区都与其他可用区中的故障隔离，并提供与同一区域其他可用区的低成本、低延迟网络连接。这使您可以在某个区域的多个可用区之间快速复制数据。 3. 当您的应用程序将某个数据写入 DynamoDB 表并收到 HTTP 200 响应 (OK) 时，该数据的所有副本都会更新。该数据最终将在所有存储位置中保持一致，通常只需一秒或更短时间。 4. 最终一致性读取 当您从 DynamoDB 表中读取数据时，响应反映的可能不是刚刚完成的写入操作的结果。响应可能包含某些陈旧数据。如果您在短时间后重复读取请求，响应将返回最新的数据。 5. 强一致性读取 当您请求强一致性读取时，DynamoDB 会返回具有最新数据的响应，从而反映来自所有已成功的之前写入操作的更新。如果网络延迟或中断，可能会无法执行强一致性读取。 读取操作 (例如 GetItem，Query 和 Scan) 提供了一个 ConsistentRead 参数。如果您将此参数设置为 true，DynamoDB 将在操作过程中使用强一致性读取。 读取和写入吞吐容量 在 Amazon DynamoDB 中创建表或索引时，必须指定读写活动的容量要求。通过提前定义您的吞吐容量，DynamoDB 可以预留必要的资源，以满足您的应用程序所需的读写活动，同时确保一致的低延迟性能。 一个 读取容量单位 表示对大小最多为 4 KB 的项目每秒执行一次强一致性读取，或每秒执行两次最终一致性读取。如果您需要读取大于 4 KB 的项目，DynamoDB 需要消耗额外的读取容量单位。所需的读取容量单位的总数取决于项目大小以及您需要最终一致性读取还是强一致性读取。 一个 写入容量单位 表示对大小最多为 1 KB 的项目每秒执行一次写入。如果您需要写入大于 1 KB 的项目，DynamoDB 需要消耗额外的写入容量单位。所需的写入容量单位的总数取决于项目大小。 例如，假设您创建了具有 5 个读取容量单位和 5 个写入容量单位的表。使用这些设置，您的应用程序可以： 1. 执行高达每秒 20KB 的强一致性读取 (4 KB × 5 个读取容量单位)。 2. 执行高达每秒 40KB 的最终一致性读取 (读取吞吐量的两倍)。 3. 每秒写入高达 5KB (1 KB × 5 个写入容量单位)。 如果您的应用程序读取或写入较大型的项目 (最大为 DynamoDB 的项目大小上限 400 KB)，它将消耗更多的容量单位。 您的读取或写入请求超过了表的吞吐量设置，则 DynamoDB 可能会限制 该请求。DynamoDB 也可以限制超过了索引吞吐量的读取请求。限制会阻止您的应用程序消耗太多容量单位。当请求受到限制时，它将失败并出现代码 HTTP 400 (Bad Request) 和一个 ProvisionedThroughputExceededException 1. 增加预置吞吐量 可以根据需要使用 AWS 管理控制台或 UpdateTable 操作增加 ReadCapacityUnits 或 WriteCapacityUnits。 2. 减少预置吞吐量 每天最多可执行 4 次调低操作. 如果过去 4 小时内未执行减小操作，则可以执行额外的调低操作 表表大小表的大小没有实际限制。表的项目数和字节数是无限制的。 每个账户中表的数量对于任何 AWS 账户，每个区域的初始限制为 256 个表。 批处理DynamoDB 低级 API 支持批量读取和写入操作.批量操作可以容忍批处理中的个别请求失败。举例来说，假设一个 BatchGetItem 请求读取五个项目。即使某些底层 GetItem 请求失败，这也不会导致整个 BatchGetItem 操作失败。另一方面，如果所有五个读取操作都失败，则整个 BatchGetItem 将失败。批量操作会返回有关各失败请求的信息，以便您诊断问题并重试操作。对于 BatchGetItem，在请求的 UnprocessedKeys 参数中会返回有问题的表和主键。对于 BatchWriteItem，在 UnprocessedItems 中返回类似信息。如果 DynamoDB 返回了任何未处理的项目，应对这些项目重试批量操作。然而，我们强烈建议您使用指数回退算法。如果立即重试批量操作，底层读取或写入请求仍然会由于各表的限制而失败。如果使用指数回退延迟批量操作，批处理中的各请求成功的可能性更大。 QueryQuery 操作基于主键值查找项目。您可查询具有复合主键 (分区键和排序键) 的任何表或二级索引。 您必须提供分区键属性的名称以及该属性的一个值。Query 将返回具有该分区键值的所有项目。您可以选择提供排序键属性，并使用比较运算符来优化搜索结果。Query 操作始终返回结果集。如果未找到完全匹配项目，则结果集将为空。Query 结果始终按排序键值排序。如果排序键的数据类型是数字，则会按数字顺序返回结果；否则，会按 UTF-8 字节的顺序返回结果。默认情况下，系统按升序排序。要颠倒顺序，请将 ScanIndexForward 参数设置为 false。单个 Query 操作最多可检索 1 MB 的数据。在向结果应用任何 FilterExpression 之前，将应用此限制。如果 LastEvaluatedKey 包含在响应中且为非 null 值，则您将需要为结果集分页 键条件表达式要指定搜索条件，请使用键条件表达式 - 用于确定要从表或索引中读取的项目的字符串。您必须指定分区键名称和值作为等式条件。分区键: 必须指定分区键名称和值作为等式条件。您可选择为排序键提供另一个条件（如果有）。排序键条件必须使用下列比较运算符之一: a = b - 如果属性 a 等于值 b，则为 true a &lt; b - 如果 a 小于 b 则为 true a &lt;= b - 如果 a 小于或等于 b 则为 true a &gt; b - 如果 a 大于 b 则为 true a &gt;= b - 如果 a 大于或等于 b 则为 true a BETWEEN b AND c - 如果 a 大于或等于 b 且小于或等于 c，则为 true。 以下函数也受支持：begins_with (a, substr) - 如果属性 a 的值以特定子字符串开头，则为 true。 筛选表达式筛选表达式在 Query 已完成但结果尚未返回时应用。因此，无论是否存在筛选表达式，Query 都将占用同等数量的读取容量。筛选表达式不得包含分区键或排序键属性。您需要在关键字条件表达式而不是筛选表达式中指定这些属性。 graph BT subgraph AWS DynamoDB D1(Apply KeyConditionExpression) D2(Apply Page Size - Limit) D3(Apply 1M Limitation) D4(Apply FilterExpression) D1 --> D2 D2 --> D3 D3 --> D4 end A[Query/Scan] --> |request|D1 D4 --> |Response|B[Result Pagination: LastEvaluatedKey & ExclusiveStartKey] B -. Next Page .- A]]></content>
      <tags>
        <tag>DynamoDB</tag>
        <tag>AWS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[gas 是什么?]]></title>
    <url>%2F2018%2F03%2F14%2Fethfans-6-trasaction-gas%2F</url>
    <content type="text"><![CDATA[gas 是以太坊(Ethereum)的计量单位, 用以计算以太链上一个或一些动作所花费的工作量.以太链上的任意交易或者智能合约的每一个操作都是需要消耗gas的, 需要耗费多计算量的操作肯定比耗费少计算量的操作所消耗的gas多 . 划重点: gas 是计算手续费的一种方法 gas 是计量工作量的, 手续费是以Ether给付的. gasCost = gasPrice * gasUsed: gasCost为一个操作所需给付的手续费, gasUsed是这个操作所消耗的工作量, gasPrice为每个工作量的价格. 打个比方: 一个家政人员的每小时价格(gasPrice), 乘以其总共工作时间(gasUsed), 就是此次家政服务总共应付工资. 以太链上优先处理高gasPrice的操作, 如果你的价格太低了, 是没有人为你工作的, 你的交易可能永远不会被处理 不管你价格的高低, 如果你提交的交易所消耗的工作量超出预算了, 这笔交易会失败, 但是这笔交易还是会被提交到链上, 而且你给付的手续费是要不回来的, 因为矿工已经为你付出了工作量! gas确保了没有任何东西可以一直在运行着, 人们会仔细思考自己的代码, 确保其正常及正确的运行. 这也保证了矿工和用户不收恶劣代码的影响. 下面我们来验证下gas的计算: 一个普通Transaction我们通过一个普通的交易来验证下gas的消耗 (测试环境是基于之前的 搭建私有链(Private Chain)并进行挖矿和交易) ) 前期准备两个账户信息如下 1234&gt; eth.getBalance('0x8b323a1fada35f1a4d770f49d572e4f6f194cf3f')210000000000000000000&gt; eth.getBalance('0xbf262a0eb1b105c06d6dfafcdecfd4ff8e037562')0 产生交易 账户1给账户2转5Wei: 1234&gt; eth.estimateGas(&#123;from:'0x8b323a1fada35f1a4d770f49d572e4f6f194cf3f',to:'0xbf262a0eb1b105c06d6dfafcdecfd4ff8e037562',value: 5&#125;)21000&gt; eth.sendTransaction(&#123;from:'0x8b323a1fada35f1a4d770f49d572e4f6f194cf3f',to:'0xbf262a0eb1b105c06d6dfafcdecfd4ff8e037562',value: 5&#125;)0x13a32a89c816879add2fa04b922dbb3b6fe50cae9ec294d7a2ba36c6e40df2b5 转账后的账户余额: 1234&gt; eth.getBalance('0x8b323a1fada35f1a4d770f49d572e4f6f194cf3f')209999621999999999995&gt; eth.getBalance('0xbf262a0eb1b105c06d6dfafcdecfd4ff8e037562')5 交易细节 1234567891011121314151617&gt; eth.getTransaction('0x13a32a89c816879add2fa04b922dbb3b6fe50cae9ec294d7a2ba36c6e40df2b5')&#123; blockHash: "0x69dbda31332ff136942f62a4011052ce8265e8b408d9c939169d5ddc17b6b75b", blockNumber: 198, from: "0x8b323a1fada35f1a4d770f49d572e4f6f194cf3f", gas: 90000, gasPrice: 18000000000, hash: "0x13a32a89c816879add2fa04b922dbb3b6fe50cae9ec294d7a2ba36c6e40df2b5", input: "0x", nonce: 0, r: "0x83c8ee2533f9d846667a4bcbe79016884ae4a434d747aa139cfbe2d1fb8855e8", s: "0x5fb8995cd4fe6ea13d6b33a56d66c449bda6cfdccc5ac39d1981aab49ba34243", to: "0xbf262a0eb1b105c06d6dfafcdecfd4ff8e037562", transactionIndex: 0, v: "0xb4", value: 5&#125; 123456789101112131415&gt; eth.getTransactionReceipt('0x13a32a89c816879add2fa04b922dbb3b6fe50cae9ec294d7a2ba36c6e40df2b5')&#123; blockHash: "0x69dbda31332ff136942f62a4011052ce8265e8b408d9c939169d5ddc17b6b75b", blockNumber: 198, contractAddress: null, cumulativeGasUsed: 21000, from: "0x8b323a1fada35f1a4d770f49d572e4f6f194cf3f", gasUsed: 21000, logs: [], logsBloom: "0x00000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000", root: "0x6bf161294dff09f1da4e936d1a6b119825ab5b24532c89e645c30cb2edb5aad4", to: "0xbf262a0eb1b105c06d6dfafcdecfd4ff8e037562", transactionHash: "0x13a32a89c816879add2fa04b922dbb3b6fe50cae9ec294d7a2ba36c6e40df2b5", transactionIndex: 0&#125; 计算交易手续费: 通过转账前后账户的余额对比, 账户1在转账后, 除了转出了5Wei, 还有一部分差额: (210000000000000000000 - 209999621999999999995) - (5 - 0) = 378000000000000, 这里的差额378000000000000就是手续费. 再通过从上面的交易详情, 我们可以看到gasUsed为21000, gasPrice为18000000000, 那么gasCost为: 21000 * 18000000000 = 378000000000000. OK, 验证完成, 数字上完全匹配: 1gasCost = 账户1的减少资产 - 账户2的增加资产 = gasUsed * gasPrice 参考 https://ethereum.stackexchange.com/questions/3/what-is-meant-by-the-term-gas https://www.cryptocompare.com/coins/guides/what-is-the-gas-in-ethereum/ http://news.91.com/all/s594b1ce56e4c.html https://bitshuo.com/topic/58cb86e80a3de8932e6f7591 https://zhuanlan.zhihu.com/p/26875541]]></content>
      <categories>
        <category>区块连</category>
      </categories>
      <tags>
        <tag>区块链</tag>
        <tag>ethereum</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[智能合约开发 - Truffle & Ganache]]></title>
    <url>%2F2018%2F03%2F09%2Fethfans-4-dev-truffle%2F</url>
    <content type="text"><![CDATA[以太坊智能合约的开发语言是Solidity. Solidity是一个可以基于web端进行开发&amp;编译, 我们选择下面的框架技术, 可以专注于业务的开发, 提高开发效率: Truffle: 基于NodeJS为Solidity提供了的开发测试环境, 是很好的选择 ganache: Truffle官方推荐的, 它是一个可以快速启动的个人以太链, 可以用于开发, 测试, 监控. 它提供了图形用户界面. 基础环境:以下安装是基于NodeJS: 安装truffle 1npm install -g truffle 安装ganache 1npm install -g ganache-cli 创建项目 如果是不需要’web’, 可以直接用下面的命令: 123mkdir dappcd dapptruffle init 有’web’的其实更好理解, drizzle 是基于ReactJS的集成智能合约的前台实现. 123mkdir dappcd dapptruffle unbox drizzle 我们用方式2来启动初始化项目. Truffle Box 提供了几个有用的样板, 让开发人员更专注于dapp的开发. 运行及测试drizzle已经提供了基础的demo, 我们可以运行起来看下效果 因为drizzle demo中默认的端口是8545, 那么我们将ganache启动在8545端口 1ganache-cli -p 8545 编译和发布智能合约 智能合约需要编译和发布到以太链上, 这里需要用到Truffle命令 12345# 编译只能合约truffle compile# 发布编译好了的智能合约到以太链上truffle migrate 启动Web UI: 运行Demo 1npm run start 效果如下: 参考: http://truffleframework.com/blog/drizzle-reactive-ethereum-data-for-front-ends http://truffleframework.com/docs/ganache/using]]></content>
      <categories>
        <category>区块连</category>
      </categories>
      <tags>
        <tag>区块链</tag>
        <tag>ethereum</tag>
        <tag>smart contract</tag>
        <tag>solidity</tag>
        <tag>truffle</tag>
        <tag>ganache</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[私有链集群]]></title>
    <url>%2F2018%2F03%2F08%2Fethfans-3-private-chain-cluster%2F</url>
    <content type="text"><![CDATA[在私有网络中建立多个节点组成的集群,并互相发现,产生交易. 创世区块集群中所有的节点必须报有相同的genesis state, 否则节点无法关联上123456789101112131415161718192021&#123; "config": &#123; "chainId": 180, "homesteadBlock": 0, "eip155Block": 0, "eip158Block": 0 &#125;, "alloc": &#123; "fad9684b077d9046ff2e293a375d29e2ebc5445a": &#123; "balance": "30000000000000000" &#125; &#125;, "coinbase": "0x0000000000000000000000000000000000000000", "difficulty": "0x20000", "extraData": "", "gasLimit": "0x2fefd8", "nonce": "0x0000000000000042", "mixhash": "0x0000000000000000000000000000000000000000000000000000000000000000", "parentHash": "0x0000000000000000000000000000000000000000000000000000000000000000", "timestamp": "0x00"&#125; 搭建私有链节点参考&lt;&lt;搭建私有链(Private Chain)并进行挖矿和交易&gt;&gt;, 启动节点1作为boot node, 1234# Node 1 - boot node 1geth --datadir ./data/1 init genesis.jsoncp -R ./keystore/ ./data/1/keystore/geth --datadir ./data/1 --identity "boot-node-1" --networkid 1808923373394438348 --verbosity 6 --ipcdisable --port 30301 --rpcport 8101 console 2&gt;&gt; ./logs/1.log 可以查看帐号信息和当前节点信息: 123456&gt; personal.listAccounts["0xfad9684b077d9046ff2e293a375d29e2ebc5445a"]&gt; admin.peers[]&gt; admin.nodeInfo.enode"enode://19c2f5b1d4e3ea0d4b042eb3b631b0eb7648c442df687926d8b7063e2d50404e1566c8a052e5c57925b20df9a8b0fa2511679df20c37100b62e6f1446cf5bfe3@[::]:30301" 启动节点3作为挖矿节点 以--mine启动节点3作为挖矿节点, 再通过--bootnodes配置关联节点1 1234567# Node 3 - miner nodegeth --datadir ./data/3 init genesis.jsoncp -R ./keystore/ ./data/3/keystore/geth --datadir ./data/3 --identity "miner-node" --networkid 1808923373394438348 --verbosity 4 --ipcdisable --port 30303 --rpcport 8103 \ --minerthreads=1 --etherbase fad9684b077d9046ff2e293a375d29e2ebc5445a \ --bootnodes "enode://19c2f5b1d4e3ea0d4b042eb3b631b0eb7648c442df687926d8b7063e2d50404e1566c8a052e5c57925b20df9a8b0fa2511679df20c37100b62e6f1446cf5bfe3@127.0.0.1:30301" \ console 2&gt;&gt; ./logs/3.log 节点3起来后,检查节点联系: 123456&gt; personal.listAccounts["0xfad9684b077d9046ff2e293a375d29e2ebc5445a"]&gt; admin.peers[&#123;...&#125;] 注意: 通过IP 127.0.0.1替代, 外部主机也可以通过外部IP建立关联. (可以通过 ifconfig|grep netmask|awk &#39;{print $2}&#39; 可获取IP.) 启动节点2也作为boot node: 1234# Node 2 - boot node 2geth --datadir ./data/2 init genesis.jsoncp -R ./keystore/ ./data/2/keystore/geth --datadir ./data/2 --identity "boot-node-2" --networkid 1808923373394438348 --verbosity 4 --ipcdisable --port 30302 --rpcport 8102 console 2&gt;&gt; ./logs/2.log 节点2起来后,可以查看帐号信息和当前节点信息: 123456&gt; personal.listAccounts["0xfad9684b077d9046ff2e293a375d29e2ebc5445a"]&gt; admin.peers[]&gt; admin.nodeInfo.enode"enode://dd4b2503ad79644d0591daa7b11bcb04ce62fe4798a1747b62a8289e3dbd0cdb770478dcc44e42a00ff2ddcfde1cfeb98fbf36769474bfcddb07704b75e0f026@[::]:30302" 关联节点2 &amp; 3: 在节点3控制台通过admin.addPeer()命令关联节点2 &amp; 3, 再检查节点1, 2 &amp; 3的peers: 123456&gt; addmin.addPeer('enode://dd4b2503ad79644d0591daa7b11bcb04ce62fe4798a1747b62a8289e3dbd0cdb770478dcc44e42a00ff2ddcfde1cfeb98fbf36769474bfcddb07704b75e0f026@127.0.0.1:30302')true&gt; admin.peers[...] 查看账余额 检查初始账户的余额为30000000000000000, 通过miner.start(1)让矿工开工: 1234&gt; miner.start(1)null&gt; eth.getBalance('0xfad9684b077d9046ff2e293a375d29e2ebc5445a')30000000000000000 金额在变化, 矿工已开工. 启动节点4为交易节点 通过--bootnodes将两个boot node都关联上: 123456# Node 4 - transaction node 1geth --datadir ./data/4 init genesis.jsoncp -R ./keystore/ ./data/4/keystore/geth --datadir ./data/4 --identity "transaction-node-1" --networkid 1808923373394438348 --verbosity 4 --ipcdisable --port 30304 --rpcport 8104 \ --bootnodes "enode://19c2f5b1d4e3ea0d4b042eb3b631b0eb7648c442df687926d8b7063e2d50404e1566c8a052e5c57925b20df9a8b0fa2511679df20c37100b62e6f1446cf5bfe3@127.0.0.1:30301,enode://dd4b2503ad79644d0591daa7b11bcb04ce62fe4798a1747b62a8289e3dbd0cdb770478dcc44e42a00ff2ddcfde1cfeb98fbf36769474bfcddb07704b75e0f026@127.0.0.1:30302" \ console 2&gt;&gt; ./logs/4.log 一个交易节点发生交易 通过miner.stop()关停矿区, 在交易节点上新加帐号, 并转账: 1234567891011121314151617181920212223242526272829303132333435363738&gt; personal.newAccount("acct1")"0xfe14b8ed857f4482602d9438f651c94e8a53b31f"&gt; eth.getBalance('0xfe14b8ed857f4482602d9438f651c94e8a53b31f')0&gt; eth.getBalance('0xfad9684b077d9046ff2e293a375d29e2ebc5445a')890029999999999999995&gt; personal.unlockAccount('0xfad9684b077d9046ff2e293a375d29e2ebc5445a')Unlock account 0xfad9684b077d9046ff2e293a375d29e2ebc5445aPassphrase: [password]true&gt; eth.sendTransaction(&#123;from:'0xfad9684b077d9046ff2e293a375d29e2ebc5445a',to:'0xfe14b8ed857f4482602d9438f651c94e8a53b31f',value:5&#125;)"0x01afeaac31ad8226950d45c91e637021546071978eb0589ecc7ba03f9b7f7302"&gt; eth.getTransaction('0x01afeaac31ad8226950d45c91e637021546071978eb0589ecc7ba03f9b7f7302') // 产看交易细节&#123; blockHash: "0x0000000000000000000000000000000000000000000000000000000000000000", blockNumber: null, from: "0xfad9684b077d9046ff2e293a375d29e2ebc5445a", gas: 90000, gasPrice: 18000000000, hash: "0x01afeaac31ad8226950d45c91e637021546071978eb0589ecc7ba03f9b7f7302", input: "0x", nonce: 0, r: "0x79b8930fa1efbf3352636b8f1723ff040fb4d477489d5e424b1da36ab799c59f", s: "0x371bbebf6d5e1f568f2169d1c581c7e79f8c92ce28d9873ec6c23095e9d6092e", to: "0xfe14b8ed857f4482602d9438f651c94e8a53b31f", transactionIndex: 0, v: "0x18b", value: 5&#125;&gt; eth.getBalance('0xfe14b8ed857f4482602d9438f651c94e8a53b31f')0&gt; eth.getBalance('0xfad9684b077d9046ff2e293a375d29e2ebc5445a')890029999999999999995&gt; txpool.status // 查看交易池&#123; pending: 1, queued: 0&#125; 矿工开工miner.start(1), 再来查看交易: 1234567891011121314151617181920212223242526&gt; txpool.status // 查看交易池&#123; pending: 0, queued: 0&#125;&gt; eth.getBalance('0xfe14b8ed857f4482602d9438f651c94e8a53b31f')5&gt; eth.getBalance('0xfad9684b077d9046ff2e293a375d29e2ebc5445a')890029999999999999990&gt; eth.getTransaction('0x01afeaac31ad8226950d45c91e637021546071978eb0589ecc7ba03f9b7f7302')&#123; blockHash: "0x2dfa70b0383b49f69ce8863ac453108f52921f0880c0fb4b4e14230cd82b5186", blockNumber: 127, from: "0xfad9684b077d9046ff2e293a375d29e2ebc5445a", gas: 90000, gasPrice: 18000000000, hash: "0x01afeaac31ad8226950d45c91e637021546071978eb0589ecc7ba03f9b7f7302", input: "0x", nonce: 0, r: "0x79b8930fa1efbf3352636b8f1723ff040fb4d477489d5e424b1da36ab799c59f", s: "0x371bbebf6d5e1f568f2169d1c581c7e79f8c92ce28d9873ec6c23095e9d6092e", to: "0xfe14b8ed857f4482602d9438f651c94e8a53b31f", transactionIndex: 0, v: "0x18b", value: 5&#125; 交易节点之间发生交易 以static node的方式关联节点, 配置 static-nodes.json 1234[ "enode://19c2f5b1d4e3ea0d4b042eb3b631b0eb7648c442df687926d8b7063e2d50404e1566c8a052e5c57925b20df9a8b0fa2511679df20c37100b62e6f1446cf5bfe3@127.0.0.1:30301", "enode://dd4b2503ad79644d0591daa7b11bcb04ce62fe4798a1747b62a8289e3dbd0cdb770478dcc44e42a00ff2ddcfde1cfeb98fbf36769474bfcddb07704b75e0f026@127.0.0.1:30302"] 启动节点5为第二个交易节点: 123456# Node 5 - transaction node 2geth --datadir ./data/5 init genesis.jsoncp -R ./keystore/ ./data/5/keystore/cp -R ./static-nodes.json ./data/5/geth --datadir ./data/5 --identity transaction-node-2 --networkid 1808923373394438348 --verbosity 4 --ipcdisable --port 30305 --rpcport 8105 \ console 2&gt;&gt; ./logs/5.log 为节点5创建帐号: 123456&gt; personal.newAccount('acct2')"0x210bb5dc90b655622ae4be01a37c26ec35b83ed2"&gt; eth.getBalance('0xfe14b8ed857f4482602d9438f651c94e8a53b31f')1650000000010&gt; eth.getBalance('0x210bb5dc90b655622ae4be01a37c26ec35b83ed2')0 从节点4开始交易: 123456&gt; personal.unlockAccount('0xfe14b8ed857f4482602d9438f651c94e8a53b31f')Unlock account 0xfe14b8ed857f4482602d9438f651c94e8a53b31fPassphrase: [password]true&gt; eth.sendTransaction(&#123;from:'0xfe14b8ed857f4482602d9438f651c94e8a53b31f',to:'0x210bb5dc90b655622ae4be01a37c26ec35b83ed2',value:5&#125;)"0x01afeaac31ad8226950d45c91e637021546071978eb0589ecc7ba03f9b7f7302" 注意: 当前节点必须包含转出账户, 否则无法交易 通过 static-nodes.json 的方式创建起来的节点, 是只能发现文件中配置的节点, 无法发现集群中中其他的节点, 这样可以用于做网络隔离. 关停boot node 当关停boot node时, 会发现两个通过boot node简历关联的节点还是可以发现彼此的. boot node在集群中充当一个审批人的角色, 一个新的节点在需要给boot node发申请, 由boot node决定它是否可以加入这个圈子. boot node同意它的申请后, 它从boot node得到集群中其他节点的信息, 然后它并会与这些节点建立联系. 这时, boot node就已经完成了他的任务, 后续它的存在是可有可无的. 疑问: 节点集群中的单个节点上创建的帐号如何同步到其他的节点上 补充节点发现协议 sequenceDiagram New Node ->> Boot Node: ping Boot Node -->> New Node: pong New Node ->> Boot Node: findnode Boot Node -->> New Node: neighbors 参考: http://blog.csdn.net/wo541075754/article/details/78926177 http://www.cnblogs.com/zl03jsj/p/6876064.html http://blog.csdn.net/ddffr/article/details/78732256 http://blog.csdn.net/ddffr/article/details/78912026 https://github.com/ethereum/wiki/wiki/Node-discovery-protocol https://github.com/ethereum/devp2p/blob/master/rlpx.md#node-discovery]]></content>
      <categories>
        <category>区块连</category>
      </categories>
      <tags>
        <tag>区块链</tag>
        <tag>ethereum</tag>
        <tag>geth</tag>
        <tag>private chain</tag>
        <tag>cluster</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[搭建私有链(Private Chain)并进行挖矿和交易]]></title>
    <url>%2F2018%2F03%2F08%2Fethfans-2-setup-private-chain%2F</url>
    <content type="text"><![CDATA[搭建私有链并进行挖矿和交易创世区块的配置搭建私有链首先需要指定创世区块的配置，此文件就是一个内容格式为json的文本文件。 我们可以将创世区块 genesis.json 的配置如下:1234567891011121314151617&#123; "alloc": &#123;&#125;, "config": &#123; "chainID": 72, "homesteadBlock": 0, "eip155Block": 0, "eip158Block": 0 &#125;, "nonce": "0x0000000000000000", "difficulty": "0x4000", "mixhash": "0x0000000000000000000000000000000000000000000000000000000000000000", "coinbase": "0x0000000000000000000000000000000000000000", "timestamp": "0x00", "parentHash": "0x0000000000000000000000000000000000000000000000000000000000000000", "extraData": "0x11bbe8db4e347b4e8c937c1c8370e4b5ed33adb3db69cbdb7a38e1e50b1b82fa", "gasLimit": "0xffffffff"&#125; 配置项简介: alloc: 用来预置账号以及账号的以太币数量，因为私有链挖矿比较容易，所以不需要预置有币的账号，需要的时候自己创建即可以。 nonce: 一个64位随机数，用于挖矿，和mixhash的设置需要满足以太坊的Yellow paper, 4.3.4.Block Header Validity (44)章节所描述的条件。 difficulty: 设置计算区块的难度，如果数值过大，挖矿时间较长，在测试环境为节省算力和等带时间可设置较小值。 mixhash: 与nonce配合用于挖矿，由上一个区块的一部分生成的hash。和nonce的设置需要满足以太坊的Yellow paper, 4.3.4. Block Header Validity, (44)章节所描述的条件。 coinbase: 矿工账号，随便填写。 timestamp: 设置创世块的时间戳。 parentHash: 上一个区块的hash值，因为是创世块，所以这个值是0。 extraData: 附加信息，随便填，可以填你的个性信息，必须为十六进制的字符串。 gasLimit: 该值设置对GAS的消耗总量限制，用来限制区块能包含的交易信息总和，因为是私有链，所以填最大。 初始化创世块 通过以下命名初始化创世区块: 1geth --datadir ./private_chain_data_1/ init genesis.json 得到日志如下:123456789INFO [03-07|21:02:11] Maximum peer count ETH=25 LES=0 total=25INFO [03-07|21:02:11] Allocated cache and file handles database=/home/wangy/ethereum/private_chain/private_chain_data_1/geth/chaindata cache=16 handles=16INFO [03-07|21:02:11] Writing custom genesis blockINFO [03-07|21:02:11] Persisted trie from memory database nodes=0 size=0.00B time=52.906µs gcnodes=0 gcsize=0.00B gctime=0s livenodes=1 livesize=0.00BINFO [03-07|21:02:11] Successfully wrote genesis state database=chaindata hash=942f59…a2588aINFO [03-07|21:02:11] Allocated cache and file handles database=/home/wangy/ethereum/private_chain/private_chain_data_1/geth/lightchaindata cache=16 handles=16INFO [03-07|21:02:11] Writing custom genesis blockINFO [03-07|21:02:11] Persisted trie from memory database nodes=0 size=0.00B time=1.416µs gcnodes=0 gcsize=0.00B gctime=0s livenodes=1 livesize=0.00BINFO [03-07|21:02:11] Successfully wrote genesis state database=lightchaindata hash=942f59…a2588a 私有链初始化完成. 启动并进入控制台 通过指定datadir来启动节点:1geth --datadir ./private_chain_data_1/ --networkid 88 --nodiscover console geth参数说明: networkid 指定网路ID，确保不适用1-4。 nodiscover 此参数确保geth不去寻找peers，主要是为了严格控制联盟链连入的节点。 identity: 区块链的标示，随便填写，用于标示目前网络的名字 init: 指定创世块文件的位置，并创建初始块 datadir: 设置当前区块链网络数据存放的位置 port: 网络监听端口 rpc: 启动rpc通信，可以进行智能合约的部署和调试 rpcapi: 设置允许连接的rpc的客户端，一般为db,eth,net,web3 console: 启动命令行模式，可以在Geth中执行命令 节点启动后, 得到日志如下:1234567891011121314151617181920INFO [03-07|21:08:40] Maximum peer count ETH=25 LES=0 total=25INFO [03-07|21:08:40] Starting peer-to-peer node instance=Geth/v1.8.2-stable/linux-amd64/go1.8.3INFO [03-07|21:08:40] Allocated cache and file handles database=/home/wangy/ethereum/private_chain/private_chain_data_1/geth/chaindata cache=768 handles=512WARN [03-07|21:08:40] Upgrading database to use lookup entriesINFO [03-07|21:08:40] Initialised chain configuration config=&quot;&#123;ChainID: 72 Homestead: 0 DAO: &lt;nil&gt; DAOSupport: false EIP150: &lt;nil&gt; EIP155: 0 EIP158: 0 Byzantium: &lt;nil&gt; Constantinople: &lt;nil&gt; Engine: unknown&#125;&quot;INFO [03-07|21:08:40] Disk storage enabled for ethash caches dir=/home/wangy/ethereum/private_chain/private_chain_data_1/geth/ethash count=3INFO [03-07|21:08:40] Disk storage enabled for ethash DAGs dir=/home/wangy/.ethash count=2INFO [03-07|21:08:40] Initialising Ethereum protocol versions=&quot;[63 62]&quot; network=88INFO [03-07|21:08:40] Loaded most recent local header number=0 hash=942f59…a2588a td=16384INFO [03-07|21:08:40] Loaded most recent local full block number=0 hash=942f59…a2588a td=16384INFO [03-07|21:08:40] Loaded most recent local fast block number=0 hash=942f59…a2588a td=16384INFO [03-07|21:08:40] Regenerated local transaction journal transactions=0 accounts=0INFO [03-07|21:08:40] Starting P2P networkingINFO [03-07|21:08:40] Database deduplication successful deduped=0INFO [03-07|21:08:40] RLPx listener up self=&quot;enode://b41177a956e39a36f77bb73b857688d3c27989a259a8820559d1efa81e8952b70280bfa91419acd7e93f2ccff5bcdb298f400bc384b6e3ce3fbf2a0952bb060b@[::]:30303?discport=0&quot;INFO [03-07|21:08:40] IPC endpoint opened url=/home/wangy/ethereum/private_chain/private_chain_data_1/geth.ipcWelcome to the Geth JavaScript console!instance: Geth/v1.8.2-stable/linux-amd64/go1.8.3 modules: admin:1.0 debug:1.0 eth:1.0 miner:1.0 net:1.0 personal:1.0 rpc:1.0 txpool:1.0 web3:1.0 通过日志可以发现这个节点启用的是默认端口为 30303. 这是一个交互式的Javascript执行环境，在这里面可以执行Javascript方法. 对象主要包括: eth: 包含一些跟操作区块链相关的方法 net: 包含以下查看p2p网络状态的方法 admin: 包含一些与管理节点相关的方法 miner: 包含启动&amp;停止挖矿的一些方法 personal: 主要包含一些管理账户的方法 txpool: 包含一些查看交易内存池的方法 web3: 包含了以上对象，还包含一些单位换算的方法 配置帐号 上一步的日志说 No etherbase set and no accounts found as default, 那么我们接着创建一个密码为password的帐号:123456789&gt; personal.listAccounts[]&gt;&gt; personal.newAccount("password")"0x1a36a03932e3b73292df56b6493058425e518857"&gt;&gt; personal.listAccounts["0x1a36a03932e3b73292df56b6493058425e518857"]&gt; 查看账户余额:12&gt; web3.fromWei(eth.getBalance(eth.coinbase), "ether")0 查看钱包信息:123456789&gt; personal.listWallets[&#123; accounts: [&#123; address: "0x1a36a03932e3b73292df56b6493058425e518857", url: "keystore:///home/wangy/ethereum/private_chain/private_chain_data_1/keystore/UTC--2018-03-08T05-16-25.359680161Z--1a36a03932e3b73292df56b6493058425e518857" &#125;], status: "Locked", url: "keystore:///home/wangy/ethereum/private_chain/private_chain_data_1/keystore/UTC--2018-03-08T05-16-25.359680161Z--1a36a03932e3b73292df56b6493058425e518857"&#125;] 挖矿帐号创建成功以后, 我们就可以通过 miner.start() 来开始挖矿了. 命令启动后, 得到日志如下:1234567891011121314&gt; miner.start()INFO [03-07|21:58:20] Updated mining threads threads=0INFO [03-07|21:58:20] Transaction pool price threshold updated price=18000000000nullINFO [03-07|21:58:20] Starting mining operationINFO [03-07|21:58:20] Commit new mining work number=1 txs=0 uncles=0 elapsed=231.751µsINFO [03-07|21:58:28] Generating DAG in progress epoch=1 percentage=0 elapsed=4.904sINFO [03-07|21:58:33] Generating DAG in progress epoch=1 percentage=1 elapsed=10.359sINFO [03-07|21:58:39] Generating DAG in progress epoch=1 percentage=2 elapsed=15.923sINFO [03-07|21:58:44] Generating DAG in progress epoch=1 percentage=3 elapsed=21.394sINFO [03-07|21:58:50] Generating DAG in progress epoch=1 percentage=4 elapsed=26.783sINFO [03-07|21:58:56] Generating DAG in progress epoch=1 percentage=5 elapsed=33.166sINFO [03-07|21:59:01] Generating DAG in progress epoch=1 percentage=6 elapsed=38.682s... 注意看 percentage, 这个时候你还没有开始挖矿哦, 在做挖矿准备工作. 只有在percentage变成了100%之后, 才可以挖到矿的. 如下, 只有到了 Generated ethash verification cache 才算整整开始挖矿. Successfully sealed new block才是挖到矿了哦12345678910111213INFO [03-07|22:07:26] Generating DAG in progress epoch=1 percentage=97 elapsed=9m2.944sINFO [03-07|22:07:31] Generating DAG in progress epoch=1 percentage=98 elapsed=9m8.134sINFO [03-07|22:07:37] Generating DAG in progress epoch=1 percentage=99 elapsed=9m13.969sINFO [03-07|22:07:37] Generated ethash verification cache epoch=1 elapsed=9m13.972sINFO [03-07|22:14:22] Successfully sealed new block number=1 hash=d26e3e…f6662fINFO [03-07|22:14:22] 🔨 mined potential block number=1 hash=d26e3e…f6662fINFO [03-07|22:14:22] Commit new mining work number=2 txs=0 uncles=0 elapsed=174.575µsINFO [03-07|22:15:00] Successfully sealed new block number=2 hash=1e4192…1f9e6eINFO [03-07|22:15:00] 🔨 mined potential block number=2 hash=1e4192…1f9e6eINFO [03-07|22:15:00] Commit new mining work number=3 txs=0 uncles=0 elapsed=119.383µsINFO [03-07|22:15:05] Successfully sealed new block number=3 hash=2f8d08…47177aINFO [03-07|22:15:05] 🔨 mined potential block number=3 hash=2f8d08…47177aINFO [03-07|22:15:05] Commit new mining work number=4 txs=0 uncles=0 elapsed=103.057µs 中途可以通过 miner.stop() 命令可以停止挖矿. 注意: 输入的字符会被挖矿刷屏信息冲掉，没有关系，只要输入完整的 miner.stop() 之后回车，即可停止挖矿。检查账户余额: 12&gt; eth.getBalance(eth.coinbase)15000000000000000000 挖矿成功. 交易新建一个帐号用于交易:1234567&gt; personal.newAccount("password1")"0x2ed11e9f064572d2f2f651e4d7c3824f0b949022"&gt; personal.newAccount("password2") "0x2935155a6980bad50a87a29b31c69adb9065cdba"&gt;&gt; personal.listAccounts["0x1a36a03932e3b73292df56b6493058425e518857", "0x2ed11e9f064572d2f2f651e4d7c3824f0b949022", "0x2935155a6980bad50a87a29b31c69adb9065cdba"] 尝试产生交易1eth.sendTransaction(&#123;from:&apos;0x1a36a03932e3b73292df56b6493058425e518857&apos;,to:&apos;0x2ed11e9f064572d2f2f651e4d7c3824f0b949022&apos;,value:5&#125;) Error: authentication needed: password or unlock at web3.js:3143:20 at web3.js:6347:15 at web3.js:5081:36 at :1:1 通过 personal.listWallets 命令可以发现掌勺是Locked状态123456789101112131415[&#123; accounts: [&#123; address: "0x1a36a03932e3b73292df56b6493058425e518857", url: "keystore:///home/wangy/ethereum/private_chain/private_chain_data_1/keystore/UTC--2018-03-08T05-16-25.359680161Z--1a36a03932e3b73292df56b6493058425e518857" &#125;], status: "Locked", url: "keystore:///home/wangy/ethereum/private_chain/private_chain_data_1/keystore/UTC--2018-03-08T05-16-25.359680161Z--1a36a03932e3b73292df56b6493058425e518857"&#125;, &#123; accounts: [&#123; address: "0x2ed11e9f064572d2f2f651e4d7c3824f0b949022", url: "keystore:///home/wangy/ethereum/private_chain/private_chain_data_1/keystore/UTC--2018-03-08T07-23-58.161695617Z--2ed11e9f064572d2f2f651e4d7c3824f0b949022" &#125;], status: "Locked", url: "keystore:///home/wangy/ethereum/private_chain/private_chain_data_1/keystore/UTC--2018-03-08T07-23-58.161695617Z--2ed11e9f064572d2f2f651e4d7c3824f0b949022"&#125;] 以太坊的保护机制，每隔一段时间帐户就会自动锁定，这个时候任何以太币在账户之间的转换都会被拒绝，除非把该账户解锁 解锁帐号:1234&gt; personal.unlockAccount(&apos;0x1a36a03932e3b73292df56b6493058425e518857&apos;)Unlock account 0x1a36a03932e3b73292df56b6493058425e518857Passphrase: [Password]true 输入密码password, 确认解锁帐号. 重新开始交易123eth.sendTransaction(&#123;from:'0x1a36a03932e3b73292df56b6493058425e518857',to:'0x2ed11e9f064572d2f2f651e4d7c3824f0b949022',value:5&#125;)INFO [03-07|23:50:03] Submitted transaction fullhash=0x6d4fb888d21b09f39f71b6e06911b090dcd1160c068605ea5ff399aef1702380 recipient=0x2ed11e9f064572d2f2f651E4d7c3824F0B949022"0x6d4fb888d21b09f39f71b6e06911b090dcd1160c068605ea5ff399aef1702380" 交易成功. 查看账户余额:1234&gt; eth.getBalance('0x1a36a03932e3b73292df56b6493058425e518857')15000000000000000000&gt; eth.getBalance('0x2ed11e9f064572d2f2f651e4d7c3824f0b949022')0 交易已完成, 怎么两个账户的余额没有发生变化呢? 查看交易池状态 txpool.status:12345&gt; txpool.status&#123; pending: 1, queued: 0&#125; 这笔交易还处于pending状态. 原来以太链上的交易是由矿工来完成的, 没有矿工, 以太链上的所有操作都是无法完成的. 矿工完成交易操作让矿工开工 miner.start().1234567INFO [03-07|23:58:14] Transaction pool price threshold updated price=18000000000null&gt; INFO [03-07|23:58:14] Starting mining operationINFO [03-07|23:58:14] Commit new mining work number=4 txs=1 uncles=0 elapsed=352.367µsINFO [03-07|23:58:57] Successfully sealed new block number=4 hash=62e46c…53ce76INFO [03-07|23:58:57] 🔨 mined potential block number=4 hash=62e46c…53ce76INFO [03-07|23:58:57] Commit new mining work number=5 txs=0 uncles=0 elapsed=1.954ms 查验交易状态1234567891011121314151617181920212223242526272829303132# 1. 检查交易池&gt; txpool.status&#123; pending: 0, queued: 0&#125;# 2. 检查账户余额&gt; eth.getBalance(&apos;0x1a36a03932e3b73292df56b6493058425e518857&apos;)129999999999999999995&gt; eth.getBalance(&apos;0x2ed11e9f064572d2f2f651e4d7c3824f0b949022&apos;)5&gt;# 3. 查看交易详情&gt; eth.getTransaction(&apos;0x6d4fb888d21b09f39f71b6e06911b090dcd1160c068605ea5ff399aef1702380&apos;)&#123; blockHash: &quot;0x62e46cef1eccf7fd5b87e22c0ea6a8d0faf085f6fe36ef450a77ae39ee53ce76&quot;, blockNumber: 4, from: &quot;0x1a36a03932e3b73292df56b6493058425e518857&quot;, gas: 90000, gasPrice: 18000000000, hash: &quot;0x6d4fb888d21b09f39f71b6e06911b090dcd1160c068605ea5ff399aef1702380&quot;, input: &quot;0x&quot;, nonce: 0, r: &quot;0xaebe9cee904684748822c069c48c3710fd18c5143146aa8cb9f5aacb3de48b14&quot;, s: &quot;0x7621ab90a1981e7a96791ebc457c3f1172a21d53fdfb3fe6c8d8a941eee8c76&quot;, to: &quot;0x2ed11e9f064572d2f2f651e4d7c3824f0b949022&quot;, transactionIndex: 0, v: &quot;0xb3&quot;, value: 5&#125; 转账成功. gas 问题 给帐号2充值 123&gt; eth.sendTransaction(&#123;from:&apos;0x1a36a03932e3b73292df56b6493058425e518857&apos;,to:&apos;0x2935155a6980bad50a87a29b31c69adb9065cdba&apos;,value: 29999999999999999995&#125;)INFO [03-08|00:20:51] Submitted transaction fullhash=0x70bbeee1f047ae71d02888b75e64d02a1f5dbba2153ffd13489844b94dca99db recipient=0x2935155a6980BAd50A87a29b31C69adb9065cdbA&quot;0x70bbeee1f047ae71d02888b75e64d02a1f5dbba2153ffd13489844b94dca99db&quot; 矿工完成交易后, 查看余额: 12345&gt; eth.getBalance(&apos;0x2ed11e9f064572d2f2f651e4d7c3824f0b949022&apos;)5&gt; eth.getBalance(&apos;0x2935155a6980bad50a87a29b31c69adb9065cdba&apos;)40000000000&gt; 帐号2给帐号1转账: 1234567&gt; personal.unlockAccount(&apos;0x2935155a6980bad50a87a29b31c69adb9065cdba&apos;)&gt; eth.sendTransaction(&#123;from:&apos;0x2935155a6980bad50a87a29b31c69adb9065cdba&apos;,to:&apos;0x2ed11e9f064572d2f2f651e4d7c3824f0b949022&apos;,value: 5&#125;) Error: insufficient funds for gas * price + value at web3.js:3143:20 at web3.js:6347:15 at web3.js:5081:36 at &lt;anonymous&gt;:1:1 WHAT! 40000000000还不够手续费的啊. 看上面的交易’0x6d4fb888d21b09f39f71b6e06911b090dcd1160c068605ea5ff399aef1702380’ 可以看出gas是90000, 而 gasPrice是18000000000. 补充以太币的单位以太币不是无限可分的, 相对于比特币只一个计量单位, 比特币的计量单位较多. 以太币的最小单位是Wei, 一个以太币等于1018 Wei.以太币有以下计量单位, 每一个单位都对应了以为大牛 ( 了解大牛 ): 单位 Wei 大牛 wei 1 Wei Dai Kwei 103 Babbage Mwei 106 Lovelace Gwei 109 Shannon Microether 1012 Szabo Milliether 1015 Finney Ether 1018 - 参考: http://blog.csdn.net/wo541075754/article/details/78926177 https://www.cnblogs.com/zl03jsj/p/6858928.html http://blog.csdn.net/ddffr/article/details/73773255 https://github.com/ethereum/go-ethereum/wiki/Private-network https://zhuanlan.zhihu.com/p/28994731]]></content>
      <categories>
        <category>区块连</category>
      </categories>
      <tags>
        <tag>区块链</tag>
        <tag>ethereum</tag>
        <tag>geth</tag>
        <tag>private chain</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[go-ethereum 安装]]></title>
    <url>%2F2018%2F03%2F08%2Fethfans-1-install%2F</url>
    <content type="text"><![CDATA[安装 (mac环境) 从源代码编译安装: 在 GitHub 下载最新的版本, 比如 v1.8.2.zip 1234unzip -d v1.8.2.zip ./go-ethereumcd go-ethereummake gethcd build/bin 将build/bin配置到path环境变量中. 不同的操作系统, 也可以借助其他的一些工具安装: 123# Homebrew on Mac OSXbrew tap ethereum/ethereumbrew install ethereum 其他环境 其他的操作系统可以参考官方文档安装: https://github.com/ethereum/go-ethereum/wiki/Building-Ethereum 测试通过下面命令打开geth的JavaScript控制台:1geth console]]></content>
      <categories>
        <category>区块连</category>
      </categories>
      <tags>
        <tag>区块链</tag>
        <tag>ethereum</tag>
        <tag>geth</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[区块链了解和学习]]></title>
    <url>%2F2018%2F03%2F07%2Fblockchain-1-introduce%2F</url>
    <content type="text"><![CDATA[比特币理解比特币的原理及运作机制: http://blog.codinglabs.org/articles/bitcoin-mechanism-make-easy.html 以太坊: go-ethereum 安装 搭建私有链(Private Chain)并进行挖矿和交易 私有链集群 智能合约开发: Truffle & Ganache gas 是什么?]]></content>
      <categories>
        <category>区块连</category>
      </categories>
      <tags>
        <tag>区块链</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Activiti (3) - 流程变量的设置于获取]]></title>
    <url>%2F2018%2F02%2F12%2Factiviti-task-variables%2F</url>
    <content type="text"><![CDATA[接着 BPMN 2.0 - Activiti, 我们以请假的工作流为例, 开始测试流程变量的设置于获取. 定义一个请假的流程 123456789101112131415161718192021222324252627282930313233343536373839404142434445&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;definitions xmlns="http://www.omg.org/spec/BPMN/20100524/MODEL" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema" xmlns:activiti="http://activiti.org/bpmn" xmlns:bpmndi="http://www.omg.org/spec/BPMN/20100524/DI" xmlns:omgdc="http://www.omg.org/spec/DD/20100524/DC" xmlns:omgdi="http://www.omg.org/spec/DD/20100524/DI" typeLanguage="http://www.w3.org/2001/XMLSchema" expressionLanguage="http://www.w3.org/1999/XPath" targetNamespace="http://www.activiti.org/test"&gt; &lt;process id="leave_process_with_variables" name="leave_process_with_variables【流程请假】" isExecutable="true"&gt; &lt;startEvent id="startevent1" name="Start"&gt;&lt;/startEvent&gt; &lt;endEvent id="endevent1" name="End"&gt;&lt;/endEvent&gt; &lt;userTask id="usertask1" name="提交申请"&gt;&lt;/userTask&gt; &lt;sequenceFlow id="flow1" sourceRef="startevent1" targetRef="usertask1"&gt;&lt;/sequenceFlow&gt; &lt;userTask id="usertask2" name="上级审批" activiti:assignee="王二"&gt;&lt;/userTask&gt; &lt;sequenceFlow id="flow2" sourceRef="usertask1" targetRef="usertask2"&gt;&lt;/sequenceFlow&gt; &lt;sequenceFlow id="flow3" sourceRef="usertask2" targetRef="endevent1"&gt;&lt;/sequenceFlow&gt; &lt;/process&gt; &lt;bpmndi:BPMNDiagram id="BPMNDiagram_leave_process_with_variables"&gt; &lt;bpmndi:BPMNPlane bpmnElement="leave_process_with_variables" id="BPMNPlane_leave_process_with_variables"&gt; &lt;bpmndi:BPMNShape bpmnElement="startevent1" id="BPMNShape_startevent1"&gt; &lt;omgdc:Bounds height="35.0" width="35.0" x="350.0" y="90.0"&gt;&lt;/omgdc:Bounds&gt; &lt;/bpmndi:BPMNShape&gt; &lt;bpmndi:BPMNShape bpmnElement="endevent1" id="BPMNShape_endevent1"&gt; &lt;omgdc:Bounds height="35.0" width="35.0" x="350.0" y="420.0"&gt;&lt;/omgdc:Bounds&gt; &lt;/bpmndi:BPMNShape&gt; &lt;bpmndi:BPMNShape bpmnElement="usertask1" id="BPMNShape_usertask1"&gt; &lt;omgdc:Bounds height="55.0" width="105.0" x="315.0" y="190.0"&gt;&lt;/omgdc:Bounds&gt; &lt;/bpmndi:BPMNShape&gt; &lt;bpmndi:BPMNShape bpmnElement="usertask2" id="BPMNShape_usertask2"&gt; &lt;omgdc:Bounds height="55.0" width="105.0" x="315.0" y="300.0"&gt;&lt;/omgdc:Bounds&gt; &lt;/bpmndi:BPMNShape&gt; &lt;bpmndi:BPMNEdge bpmnElement="flow1" id="BPMNEdge_flow1"&gt; &lt;omgdi:waypoint x="367.0" y="125.0"&gt;&lt;/omgdi:waypoint&gt; &lt;omgdi:waypoint x="367.0" y="190.0"&gt;&lt;/omgdi:waypoint&gt; &lt;/bpmndi:BPMNEdge&gt; &lt;bpmndi:BPMNEdge bpmnElement="flow2" id="BPMNEdge_flow2"&gt; &lt;omgdi:waypoint x="367.0" y="245.0"&gt;&lt;/omgdi:waypoint&gt; &lt;omgdi:waypoint x="367.0" y="300.0"&gt;&lt;/omgdi:waypoint&gt; &lt;/bpmndi:BPMNEdge&gt; &lt;bpmndi:BPMNEdge bpmnElement="flow3" id="BPMNEdge_flow3"&gt; &lt;omgdi:waypoint x="367.0" y="355.0"&gt;&lt;/omgdi:waypoint&gt; &lt;omgdi:waypoint x="367.0" y="420.0"&gt;&lt;/omgdi:waypoint&gt; &lt;/bpmndi:BPMNEdge&gt; &lt;/bpmndi:BPMNPlane&gt; &lt;/bpmndi:BPMNDiagram&gt;&lt;/definitions&gt; 发布工作流程 12345678910111213141516171819202122/** * 发布一个流程 * * @throws IOException exception */ @Test public void test2DeployTask() throws IOException &#123; myService.createDynamicWorkflow(WORKFLOW_PROCESS_DEPLOYMENT_NAME, BPMNResourceType.BPMN_20, bpmnXML); // search workflow process List&lt;Deployment&gt; deployments = myService.findDeploymentByDeploymentName(WORKFLOW_PROCESS_DEPLOYMENT_NAME); Assert.assertEquals(1, deployments.size()); deployments = myService.findDeploymentByProcessDefinitionKey(WORKFLOW_PROCESS_DEFINITION_KEY); Assert.assertEquals(1, deployments.size()); // get last deployment Deployment deployment = deployments.get(0); List&lt;String&gt; resourceNames = myService.findDeploymentResourceNamesByDeploymentId(deployment.getId()); printResources(deployment.getId(), resourceNames); &#125; 启动工作流程, 创建工作流实例: 12345String assignee = "Tom";ProcessInstance processInstance = myService.startProcess(WORKFLOW_PROCESS_DEFINITION_KEY, assignee);String processInstanceId = processInstance.getId();logger.info("processInstanceId:" + processInstance.getId()); 获取分配给当前用户的任务: 12345678// 获取TaskService服务对象的实例List&lt;Task&gt; tasks = myService.getTasks(assignee);Assert.assertEquals(1, tasks.size());Task task = tasks.get(0);logger.info("taskName:" + task.getName());logger.info("taskAssignee:" + task.getAssignee());Assert.assertEquals("提交申请", task.getName()); 设置流程变量 12345678int days = 7;Date date = new Date();String reason = "回家过年";// 1.设置流程变量，使用基本数据类型myService.setTaskVariable(task.getId(), "请假天数", days); // 与任务ID邦德myService.setTaskVariable(task.getId(), "请假日期", date);myService.setTaskVariable(task.getId(), "请假原因", reason); 获取流程变量 1234Map&lt;String, Object&gt; taskVariables = myService.getTaskVariables(task.getId());Assert.assertEquals(taskVariables.get("请假天数"), days);Assert.assertEquals(taskVariables.get("请假日期"), date);Assert.assertEquals(taskVariables.get("请假原因"), reason); 这两个步骤的可以给流程设置一些 参考: http://blog.csdn.net/caoyue_new/article/details/52171472]]></content>
      <categories>
        <category>bpmn - activiti</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>bpmn20</tag>
        <tag>spring boot</tag>
        <tag>activiti</tag>
        <tag>workflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Activiti (2) - 动态分配流程任务执行人]]></title>
    <url>%2F2018%2F02%2F04%2Factiviti-dynamic-assignee%2F</url>
    <content type="text"><![CDATA[接着 BPMN 2.0 - Activiti, 开始测试动态分配流程任务执行人. 方式一 添加一个单个task级别的listener: ManagerTaskListenerHandler 1234567public class ManagerTaskListenerHandler implements TaskListener &#123; @Override public void notify(DelegateTask delegateTask) &#123; String assignee = "Tom"; delegateTask.setAssignee(assignee); &#125;&#125; 创建工作流, 并配置taskListener如下: 1234567891011121314151617181920212223242526272829303132333435363738394041&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;definitions xmlns="http://www.omg.org/spec/BPMN/20100524/MODEL" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema" xmlns:activiti="http://activiti.org/bpmn" xmlns:bpmndi="http://www.omg.org/spec/BPMN/20100524/DI" xmlns:omgdc="http://www.omg.org/spec/DD/20100524/DC" xmlns:omgdi="http://www.omg.org/spec/DD/20100524/DI" typeLanguage="http://www.w3.org/2001/XMLSchema" expressionLanguage="http://www.w3.org/1999/XPath" targetNamespace="http://www.activiti.org/test"&gt; &lt;process id="userDynamicTask2" name="动态任务处理2" isExecutable="true"&gt; &lt;startEvent id="startevent1" name="Start"&gt;&lt;/startEvent&gt; &lt;endEvent id="endevent1" name="End"&gt;&lt;/endEvent&gt; &lt;userTask id="休假" name="休假"&gt; &lt;extensionElements&gt; &lt;activiti:taskListener event="create" class="org.wangy.bpmn.demo.v1.bpmn.ManagerTaskListenerHandler"&gt;&lt;/activiti:taskListener&gt; &lt;/extensionElements&gt; &lt;/userTask&gt; &lt;sequenceFlow id="flow1" sourceRef="startevent1" targetRef="休假"&gt;&lt;/sequenceFlow&gt; &lt;sequenceFlow id="flow2" sourceRef="休假" targetRef="endevent1"&gt;&lt;/sequenceFlow&gt; &lt;/process&gt; &lt;bpmndi:BPMNDiagram id="BPMNDiagram_userDynamicTask2"&gt; &lt;bpmndi:BPMNPlane bpmnElement="userDynamicTask2" id="BPMNPlane_userDynamicTask2"&gt; &lt;bpmndi:BPMNShape bpmnElement="startevent1" id="BPMNShape_startevent1"&gt; &lt;omgdc:Bounds height="35.0" width="35.0" x="0.0" y="10.0"&gt;&lt;/omgdc:Bounds&gt; &lt;/bpmndi:BPMNShape&gt; &lt;bpmndi:BPMNShape bpmnElement="endevent1" id="BPMNShape_endevent1"&gt; &lt;omgdc:Bounds height="35.0" width="35.0" x="268.0301705943553" y="9.999999999999968"&gt;&lt;/omgdc:Bounds&gt; &lt;/bpmndi:BPMNShape&gt; &lt;bpmndi:BPMNShape bpmnElement="休假" id="BPMNShape_休假"&gt; &lt;omgdc:Bounds height="55.0" width="105.0" x="99.01508529717765" y="0.0"&gt;&lt;/omgdc:Bounds&gt; &lt;/bpmndi:BPMNShape&gt; &lt;bpmndi:BPMNEdge bpmnElement="flow1" id="BPMNEdge_flow1"&gt; &lt;omgdi:waypoint x="357.0" y="145.0"&gt;&lt;/omgdi:waypoint&gt; &lt;omgdi:waypoint x="357.0" y="180.0"&gt;&lt;/omgdi:waypoint&gt; &lt;/bpmndi:BPMNEdge&gt; &lt;bpmndi:BPMNEdge bpmnElement="flow2" id="BPMNEdge_flow2"&gt; &lt;omgdi:waypoint x="357.0" y="235.0"&gt;&lt;/omgdi:waypoint&gt; &lt;omgdi:waypoint x="357.0" y="270.0"&gt;&lt;/omgdi:waypoint&gt; &lt;/bpmndi:BPMNEdge&gt; &lt;/bpmndi:BPMNPlane&gt; &lt;/bpmndi:BPMNDiagram&gt;&lt;/definitions&gt; 发布上述工作流 1myService.createDynamicWorkflow("test import xml workflow 2", BPMNResourceType.BPMN_20, bpmnXML); 测试 - 任务被自动分配给了”Tom” 123456789101112131415161718192021222324252627@Test public void test2UserTask() &#123; // 获取RuntimeService服务对象的实例 String processDefinitionKey = "userDynamicTask2"; // 自动执行与Key相对应的流程的最高版本 ProcessInstance processInstance = myService.startProcess(processDefinitionKey); String processInstanceId = processInstance.getId(); logger.info("processInstanceId:" + processInstance.getId()); // 获取TaskService服务对象的实例 String assignee = "Tom"; List&lt;Task&gt; tasks = myService.getTasks(assignee); Assert.assertEquals(1, tasks.size()); // 在ManagerTaskListenerHandler中, task已经本全部分配给了"Tom" Task task = tasks.get(0); logger.info("taskName:" + task.getName()); logger.info("taskAssignee:" + task.getAssignee()); Assert.assertEquals("休假", task.getName()); // 完成任务 myService.completeTask(task.getId()); // 检查结束状态 ProcessInstance pInstance = myService.findProcessInstance(processInstanceId); Assert.assertNull(pInstance); logger.info("动态任务处理流程2,使用自定义任务分配处理器方式成功执行！"); &#125; 测试 - 分配给其他人的任务也会被自动分配给”Tom” 12345678910111213141516171819202122232425262728293031@Test public void test3UserTask() &#123; // 获取RuntimeService服务对象的实例 String processDefinitionKey = "userDynamicTask2"; // 自动执行与Key相对应的流程的最高版本 String assignee = "Jeff"; ProcessInstance processInstance = myService.startProcess(processDefinitionKey, assignee); String processInstanceId = processInstance.getId(); logger.info("processInstanceId:" + processInstance.getId()); // 获取TaskService服务对象的实例 List&lt;Task&gt; tasks = myService.getTasks(assignee); Assert.assertEquals(0, tasks.size()); // 在ManagerTaskListenerHandler中, task已经本重新分配给分配给了"Tom"了 assignee = "Tom"; tasks = myService.getTasks(assignee); Assert.assertEquals(1, tasks.size()); // 在ManagerTaskListenerHandler中, task已经本全部分配给了"Tom" Task task = tasks.get(0); logger.info("taskName:" + task.getName()); logger.info("taskAssignee:" + task.getAssignee()); Assert.assertEquals("休假", task.getName()); // 完成任务 myService.completeTask(task.getId()); // 检查结束状态 ProcessInstance pInstance = myService.findProcessInstance(processInstanceId); Assert.assertNull(pInstance); logger.info("动态任务处理流程2,使用自定义任务分配处理器方式成功执行！"); &#125; 方式二 采用上例中的 123456789101112131415161718192021222324252627282930313233343536&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;definitions xmlns="http://www.omg.org/spec/BPMN/20100524/MODEL" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema" xmlns:activiti="http://activiti.org/bpmn" xmlns:bpmndi="http://www.omg.org/spec/BPMN/20100524/DI" xmlns:omgdc="http://www.omg.org/spec/DD/20100524/DC" xmlns:omgdi="http://www.omg.org/spec/DD/20100524/DI" typeLanguage="http://www.w3.org/2001/XMLSchema" expressionLanguage="http://www.w3.org/1999/XPath" targetNamespace="http://www.activiti.org/test"&gt; &lt;process id="userDynamicTask1" name="动态任务处理1" isExecutable="true"&gt; &lt;startEvent id="startevent1" name="Start"&gt;&lt;/startEvent&gt; &lt;userTask id="休假" name="休假" activiti:assignee="$&#123;person&#125;"&gt;&lt;/userTask&gt; &lt;sequenceFlow id="flow1" sourceRef="startevent1" targetRef="休假"&gt;&lt;/sequenceFlow&gt; &lt;endEvent id="endevent1" name="End"&gt;&lt;/endEvent&gt; &lt;sequenceFlow id="flow2" sourceRef="休假" targetRef="endevent1"&gt;&lt;/sequenceFlow&gt; &lt;/process&gt; &lt;bpmndi:BPMNDiagram id="BPMNDiagram_userDynamicTask1"&gt; &lt;bpmndi:BPMNPlane bpmnElement="userDynamicTask1" id="BPMNPlane_userDynamicTask1"&gt; &lt;bpmndi:BPMNShape bpmnElement="startevent1" id="BPMNShape_startevent1"&gt; &lt;omgdc:Bounds height="35.0" width="35.0" x="189.5" y="194.5"&gt;&lt;/omgdc:Bounds&gt; &lt;/bpmndi:BPMNShape&gt; &lt;bpmndi:BPMNShape bpmnElement="休假" id="BPMNShape_休假"&gt; &lt;omgdc:Bounds height="55.0" width="105.0" x="155.5" y="276.5"&gt;&lt;/omgdc:Bounds&gt; &lt;/bpmndi:BPMNShape&gt; &lt;bpmndi:BPMNShape bpmnElement="endevent1" id="BPMNShape_endevent1"&gt; &lt;omgdc:Bounds height="35.0" width="35.0" x="191.5" y="398.5"&gt;&lt;/omgdc:Bounds&gt; &lt;/bpmndi:BPMNShape&gt; &lt;bpmndi:BPMNEdge bpmnElement="flow1" id="BPMNEdge_flow1"&gt; &lt;omgdi:waypoint x="357.0" y="145.0"&gt;&lt;/omgdi:waypoint&gt; &lt;omgdi:waypoint x="357.0" y="180.0"&gt;&lt;/omgdi:waypoint&gt; &lt;/bpmndi:BPMNEdge&gt; &lt;bpmndi:BPMNEdge bpmnElement="flow2" id="BPMNEdge_flow2"&gt; &lt;omgdi:waypoint x="357.0" y="235.0"&gt;&lt;/omgdi:waypoint&gt; &lt;omgdi:waypoint x="357.0" y="270.0"&gt;&lt;/omgdi:waypoint&gt; &lt;/bpmndi:BPMNEdge&gt; &lt;/bpmndi:BPMNPlane&gt; &lt;/bpmndi:BPMNDiagram&gt;&lt;/definitions&gt; 添加一个全局Listener: 1234567891011121314151617181920212223242526272829303132333435363738public class TaskListenerTestHandler implements ActivitiEventListener &#123; //~ Static fields/initializers --------------------------------------------------------------------------------------- /** TODO: DOCUMENT ME! */ public static final Logger LOGGER = LoggerFactory.getLogger(TaskListenerTestHandler.class); //~ Methods ---------------------------------------------------------------------------------------------------------- /** * @see org.activiti.engine.delegate.event.ActivitiEventListener#isFailOnException() */ @Override public boolean isFailOnException() &#123; return false; &#125; //~ ------------------------------------------------------------------------------------------------------------------ /** * @see org.activiti.engine.delegate.event.ActivitiEventListener#onEvent(org.activiti.engine.delegate.event.ActivitiEvent) */ @Override public void onEvent(ActivitiEvent event) &#123; LOGGER.info("Activiti Event --&gt; type: &#123;&#125;, process instance: &#123;&#125;", event.getType(), event.getProcessInstanceId()); switch (event.getType()) &#123; case TASK_CREATED: &#123; ActivitiEntityEventImpl evt = (ActivitiEntityEventImpl) event; String assignee = "Tom"; TaskEntity entity = (TaskEntity) evt.getEntity(); entity.setAssignee(assignee); break; &#125; default: &#123; &#125; &#125; &#125;&#125; 配置Listener 12345678910@Autowired public void init(ProcessEngineConfigurationImpl processEngineConfiguration) &#123; List&lt;ActivitiEventListener&gt; eventListeners = processEngineConfiguration.getEventListeners(); if (Objects.isNull(eventListeners)) &#123; eventListeners = new ArrayList&lt;&gt;(); processEngineConfiguration.setEventListeners(eventListeners); &#125; eventListeners.add(new TaskListenerTestHandler()); &#125; 测试, 测试代码和例子二中的 第4步 和 第5步 一样 参考: http://blog.csdn.net/wei763328075qq/article/details/51596621]]></content>
      <categories>
        <category>bpmn - activiti</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>bpmn20</tag>
        <tag>spring boot</tag>
        <tag>activiti</tag>
        <tag>workflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Activiti (1) - 动态发布工作流]]></title>
    <url>%2F2018%2F02%2F03%2Factiviti-dynamic-deploy%2F</url>
    <content type="text"><![CDATA[很多实际业务功能需求中, 我们无法一开始预先定义好工作流程, 工作流程在可能需要及时定义, 所以我们就需要动态创建工作流程, 及动态发布.接着 BPMN 2.0 - Activiti, 开始测试动态发布工作流. service层方法 12345678910111213141516171819202122232425262728293031@Service public class MyService &#123; @Autowired private PersonRepository personRepository; @Autowired private RepositoryService repositoryService; @Autowired private RuntimeService runtimeService; @Autowired private TaskService taskService; public void createDynamicWorkflow(String name, String bpmnResourceType, String bpmnXMLString) &#123; DeploymentBuilder builder = repositoryService.createDeployment(); // 加载发布资源 builder.name(name) // 设置流程显示别名 .addString(bpmnResourceType, bpmnXMLString); // 设置流程规则文件 // 发布流程 builder.deploy(); &#125; public ProcessInstance findProcessInstance(String processInstanceId) &#123; return runtimeService.createProcessInstanceQuery().processInstanceId(processInstanceId).singleResult(); &#125; public List&lt;Task&gt; getTasks(String assignee) &#123; return taskService.createTaskQuery().taskAssignee(assignee).list(); &#125; public ProcessInstance startProcess(String processDefinitionKey, String assignee) &#123; Map&lt;String, Object&gt; variables = new HashMap&lt;String, Object&gt;(); variables.put("person", assignee); return runtimeService.startProcessInstanceByKey(processDefinitionKey, variables); &#125; BPMN的XML定义如下: 123456789101112131415161718192021222324252627282930313233343536&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;definitions xmlns="http://www.omg.org/spec/BPMN/20100524/MODEL" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema" xmlns:activiti="http://activiti.org/bpmn" xmlns:bpmndi="http://www.omg.org/spec/BPMN/20100524/DI" xmlns:omgdc="http://www.omg.org/spec/DD/20100524/DC" xmlns:omgdi="http://www.omg.org/spec/DD/20100524/DI" typeLanguage="http://www.w3.org/2001/XMLSchema" expressionLanguage="http://www.w3.org/1999/XPath" targetNamespace="http://www.activiti.org/test"&gt; &lt;process id="userDynamicTask1" name="动态任务处理1" isExecutable="true"&gt; &lt;startEvent id="startevent1" name="Start"&gt;&lt;/startEvent&gt; &lt;userTask id="休假" name="休假" activiti:assignee="$&#123;person&#125;"&gt;&lt;/userTask&gt; &lt;sequenceFlow id="flow1" sourceRef="startevent1" targetRef="休假"&gt;&lt;/sequenceFlow&gt; &lt;endEvent id="endevent1" name="End"&gt;&lt;/endEvent&gt; &lt;sequenceFlow id="flow2" sourceRef="休假" targetRef="endevent1"&gt;&lt;/sequenceFlow&gt; &lt;/process&gt; &lt;bpmndi:BPMNDiagram id="BPMNDiagram_userDynamicTask1"&gt; &lt;bpmndi:BPMNPlane bpmnElement="userDynamicTask1" id="BPMNPlane_userDynamicTask1"&gt; &lt;bpmndi:BPMNShape bpmnElement="startevent1" id="BPMNShape_startevent1"&gt; &lt;omgdc:Bounds height="35.0" width="35.0" x="189.5" y="194.5"&gt;&lt;/omgdc:Bounds&gt; &lt;/bpmndi:BPMNShape&gt; &lt;bpmndi:BPMNShape bpmnElement="休假" id="BPMNShape_休假"&gt; &lt;omgdc:Bounds height="55.0" width="105.0" x="155.5" y="276.5"&gt;&lt;/omgdc:Bounds&gt; &lt;/bpmndi:BPMNShape&gt; &lt;bpmndi:BPMNShape bpmnElement="endevent1" id="BPMNShape_endevent1"&gt; &lt;omgdc:Bounds height="35.0" width="35.0" x="191.5" y="398.5"&gt;&lt;/omgdc:Bounds&gt; &lt;/bpmndi:BPMNShape&gt; &lt;bpmndi:BPMNEdge bpmnElement="flow1" id="BPMNEdge_flow1"&gt; &lt;omgdi:waypoint x="357.0" y="145.0"&gt;&lt;/omgdi:waypoint&gt; &lt;omgdi:waypoint x="357.0" y="180.0"&gt;&lt;/omgdi:waypoint&gt; &lt;/bpmndi:BPMNEdge&gt; &lt;bpmndi:BPMNEdge bpmnElement="flow2" id="BPMNEdge_flow2"&gt; &lt;omgdi:waypoint x="357.0" y="235.0"&gt;&lt;/omgdi:waypoint&gt; &lt;omgdi:waypoint x="357.0" y="270.0"&gt;&lt;/omgdi:waypoint&gt; &lt;/bpmndi:BPMNEdge&gt; &lt;/bpmndi:BPMNPlane&gt; &lt;/bpmndi:BPMNDiagram&gt;&lt;/definitions&gt; 通过代码动态部署工作流 1myService.createDynamicWorkflow("test import xml workflow", "bpmn20.xml", bpmnXML); 上述代码让流程引擎将工作流动态部署上去了, 下面个我们接着测试运行新部署的工作流: 1234567891011121314151617181920212223242526@Test public void test2UserTask() &#123; // 获取RuntimeService服务对象的实例 String processDefinitionKey = "userDynamicTask1"; String assignee = "trademakers"; ProcessInstance processInstance = myService.startProcess(processDefinitionKey, assignee); String processInstanceId = processInstance.getId(); logger.info("processInstanceId:" + processInstance.getId()); // 获取TaskService服务对象的实例 List&lt;Task&gt; tasks = myService.getTasks(assignee); Assert.assertEquals(1, tasks.size()); Task task = tasks.get(0); logger.info("taskName:" + task.getName()); logger.info("taskAssignee:" + task.getAssignee()); Assert.assertEquals("休假", task.getName()); // 完成任务 myService.completeTask(task.getId()); // 检查结束状态 ProcessInstance pInstance = myService.findProcessInstance(processInstanceId); Assert.assertNull(pInstance); logger.info("动态任务处理流程1,使用$&#123;流程变量的Key&#125;方式成功执行！");&#125; 测试完成. 参考: http://www.kafeitu.me/activiti/2013/05/27/dynamic-process-creation-and-deployment-in-100-lines.html]]></content>
      <categories>
        <category>bpmn - activiti</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>bpmn20</tag>
        <tag>spring boot</tag>
        <tag>activiti</tag>
        <tag>workflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[BPMN 2.0 - Activiti Setup]]></title>
    <url>%2F2018%2F02%2F03%2Factiviti-setup%2F</url>
    <content type="text"><![CDATA[项目需要, 需要将BPMN 2.0应用到项目中. 参考了下现有的BPMN 2.0标准的一些现有在流行的框架技术, 选择了 Activiti 进行学习. 项目构建以spring-boot加上Activiti构建项目 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114&lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;1.5.10.RELEASE&lt;/version&gt; &lt;/parent&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.activiti&lt;/groupId&gt; &lt;artifactId&gt;activiti-spring-boot-starter-basic&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.activiti&lt;/groupId&gt; &lt;artifactId&gt;activiti-spring-boot-starter-jpa&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!--&lt;dependency&gt;--&gt; &lt;!--&lt;groupId&gt;org.activiti&lt;/groupId&gt;--&gt; &lt;!--&lt;artifactId&gt;activiti-spring-boot-starter-rest-api&lt;/artifactId&gt;--&gt; &lt;!--&lt;/dependency&gt;--&gt; &lt;dependency&gt; &lt;groupId&gt;org.activiti&lt;/groupId&gt; &lt;artifactId&gt;activiti-spring-boot-starter-actuator&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!--&lt;dependency&gt;--&gt; &lt;!--&lt;groupId&gt;com.h2database&lt;/groupId&gt;--&gt; &lt;!--&lt;artifactId&gt;h2&lt;/artifactId&gt;--&gt; &lt;!--&lt;/dependency&gt;--&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.subethamail&lt;/groupId&gt; &lt;artifactId&gt;subethasmtp-wiser&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.activiti&lt;/groupId&gt; &lt;artifactId&gt;activiti-spring-boot-starter-basic&lt;/artifactId&gt; &lt;version&gt;$&#123;activiti.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.activiti&lt;/groupId&gt; &lt;artifactId&gt;activiti-spring-boot-starter-jpa&lt;/artifactId&gt; &lt;version&gt;$&#123;activiti.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.activiti&lt;/groupId&gt; &lt;artifactId&gt;activiti-spring-boot-starter-rest-api&lt;/artifactId&gt; &lt;!--&lt;artifactId&gt;spring-boot-starter-rest-api&lt;/artifactId&gt;--&gt; &lt;version&gt;$&#123;activiti.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.activiti&lt;/groupId&gt; &lt;artifactId&gt;activiti-spring-boot-starter-actuator&lt;/artifactId&gt; &lt;version&gt;$&#123;activiti.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.h2database&lt;/groupId&gt; &lt;artifactId&gt;h2&lt;/artifactId&gt; &lt;version&gt;1.4.196&lt;/version&gt; &lt;/dependency&gt; &lt;!-- Testing --&gt; &lt;dependency&gt; &lt;groupId&gt;org.subethamail&lt;/groupId&gt; &lt;artifactId&gt;subethasmtp-wiser&lt;/artifactId&gt; &lt;version&gt;1.2&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;javax.servlet&lt;/groupId&gt; &lt;artifactId&gt;servlet-api&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;activiti.version&gt;6.0.0&lt;/activiti.version&gt; &lt;test.db.host&gt;localhost&lt;/test.db.host&gt; &lt;test.db.port&gt;3306&lt;/test.db.port&gt; &lt;test.db.name&gt;bpmn_demo_v1&lt;/test.db.name&gt; &lt;test.db.username&gt;root&lt;/test.db.username&gt; &lt;test.db.password&gt;password&lt;/test.db.password&gt; &lt;/properties&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; 数据源配置 (application.yml):1234567891011spring: jpa: hibernate: ddl-auto: update datasource: url: jdbc:mysql://@test.db.host@:@test.db.port@/@test.db.name@?createDatabaseIfNotExist=true driver-class-name: com.mysql.jdbc.Driver username: @test.db.username@ password: @test.db.password@` Java Config项目配置1234567@ComponentScan(&#123; "org.wangy.bpmn.demo.v1.service", "org.wangy.bpmn.demo.v1.controller" &#125;)@Configuration@EnableJpaRepositories("org.wangy.bpmn.demo.v1.repository")@EntityScan("org.wangy.bpmn.demo.v1.domain")public class LaunchConfig &#123; ...&#125; 接下来, 我们通过一些例子来测试工作流 - [动态发布工作流](./activiti-dynamic-deploy.md) - [动态分配流程任务执行人](./activiti-dynamic-assignee.md)]]></content>
      <categories>
        <category>bpmn - activiti</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>bpmn20</tag>
        <tag>spring boot</tag>
        <tag>activiti</tag>
        <tag>workflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Internationalization in Vue.js]]></title>
    <url>%2F2017%2F12%2F12%2Fvuejs-i18n%2F</url>
    <content type="text"><![CDATA[Internationalization in Vue.js在Vue.js里头实现国际化, 我们采用的是插件: vue-i18n 插件基础用法 引入及应用插件vue-i18n 1234import Vue from 'vue'import VueI18n from 'vue-i18n'Vue.use(VueI18n) 导入国际化资源配置 12345678910111213141516171819// Ready translated locale messagesconst messages = &#123; en: &#123; message: &#123; hello: 'hello world' &#125; &#125;, ja: &#123; message: &#123; hello: 'こんにちは、世界' &#125; &#125;&#125;// Create VueI18n instance with optionsconst i18n = new VueI18n(&#123; locale: 'ja', // set locale messages, // set locale messages&#125;) 创建Vue实例, 并应用vue-i18n配置 1new Vue(&#123; i18n &#125;) ... 使用 123&lt;el-button type="primary" class="login-btn" @click.native.prevent="submitLogin" :loading="logining"&gt; &#123;&#123; $t('button.login') &#125;&#125; &lt;/el-button&gt; 系统集成后的用法 lang目录存放国际化资源信息. 国际化文本存方于_[locale]_.json文件, 并且需要在 index.js 文件中注册. 注册方式: 123456789import cn from './cn.json'import en from './en.json'...const languages = &#123; cn: &#123;language: "简体中文", message: cn&#125;, en: &#123;language: "English", message: en&#125;&#125;;... 国际化资源文本的使用有以下三种方式:12345678910111213&lt;!-- 方式一 --&gt;&lt;el-button type="primary" class="login-btn" @click.native.prevent="submitLogin" :loading="logining"&gt;&#123;&#123; $t('button.login') &#125;&#125;&lt;/el-button&gt;&lt;!-- 方式二 --&gt;&lt;el-input type="password" v-model="loginForm.password" :placeholder="$t('login.password')" /&gt;&lt;!-- 方式三 --&gt;submitLogin(ev) &#123; let i18n = this.$i18n; alert(i18n.t('login.validation.username.require');&#125; 具体参考: vue-i18n]]></content>
      <categories>
        <category>tools</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>wkhtmltopdf</tag>
        <tag>converter</tag>
        <tag>html</tag>
        <tag>pdf</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[wkhtmltopdf - 将HTML转成PDF]]></title>
    <url>%2F2012%2F12%2F12%2Fwkhtmltopdf%2F</url>
    <content type="text"><![CDATA[介绍wkhtmltopdf是一个开源的C语言开发的命令行工具, 它通过Qt WebKit渲染引擎将HTML转换成PDF, 它可以完全不依赖于图形用户界面, 例如它在Linux Server上运行可以不依赖于X11. 官网: 请输入链接描述 命令行运行方式:1wkhtmltopdf http://google.com google.pdf 更多的运行参数参考: https://wkhtmltopdf.org/usage/wkhtmltopdf.txt Java工具类: 因为wkhtmltopdf是一个简单易用的命令行工具, 我们可以简单通过命令很简单调用它为帮助转换PDF. 以下是一个简单的Java工具类, 帮助调用wkhtmltopdf转换HTML Java 类123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245import java.io.BufferedReader;import java.io.IOException;import java.io.InputStream;import java.io.InputStreamReader;import java.nio.charset.Charset;import java.util.ArrayList;import java.util.List;import org.slf4j.Logger;import org.slf4j.LoggerFactory;/** * Document for new class. * * @User: Wang Yang * @Date: 12-12-2 * @Time: PM3:10 * @author $author$ * @version $Revision$, $Date$ */public class WKHtml2PdfConverter &#123; // 配置最大retry次数 private static final Integer MAX_RETRY_COUNT = 5; /** * wkhtmltopdf 命令位置&lt;br/&gt; * 比如 : * * &lt;pre&gt; CentOS: /opt/wkhtmltopdf/bin/wkhtmltopdf MacOS: /Users/username/Appliactions/wkhtmltopdf.app/Contents/MacOS/wkhtmltopdf &lt;/pre&gt; */ private String cmdPath; /** * wkhtmltopdf的运行参数 (https://wkhtmltopdf.org/usage/wkhtmltopdf.txt)&lt;br/&gt; * 比如设置页宽: --Letter */ private String extraArgs; /** 需要转换的HTML文件的位置. */ private String input; private final transient Logger logger = LoggerFactory.getLogger(getClass()); /** 转换后的输出PDF文件的位置. */ private String output; /** * 老版本的wkhtmltopdf是需要X11的, 在X11的运行参数类似: * * &lt;pre&gt; /usr/bin/xvfb-run --auto-servernum --server-num=1 /usr/bin/wkhtmltopdf --use-xserver --help * &lt;/pre&gt; */ private String runAsArgs; /** * 拼凑命令脚本, &#123;@link ProcessBuilder&#125;将&#123;@code input&#125;转换成&#123;@code output&#125;. * * @throws IOException DOCUMENT ME! * @throws InterruptedException DOCUMENT ME! * @throws RuntimeException DOCUMENT ME! */ public void convert() &#123; List&lt;String&gt; command = new ArrayList&lt;String&gt;(); if ((runAsArgs != null) &amp;&amp; !"".equals(runAsArgs.trim())) &#123; String[] arguments = runAsArgs.trim().split("\\s+"); for (String arg : arguments) &#123; command.add(arg); &#125; &#125; command.add(cmdPath); if ((extraArgs != null) &amp;&amp; !"".equals(extraArgs.trim())) &#123; String[] arguments = extraArgs.trim().split("\\s+"); for (String arg : arguments) &#123; command.add(arg); &#125; &#125; command.add(input); command.add(output); int retryCount = 0; boolean failed = false; do &#123; try &#123; if (failed) &#123; retryCount++; logger.info("Retry to converter the html. [Retry Times: " + retryCount + "]"); &#125; ProcessBuilder processBuilder = new ProcessBuilder(command); if (logger.isDebugEnabled()) &#123; String msg = logExecutingCommand(processBuilder); logger.debug(msg); &#125; Process process = processBuilder.start(); InputStream is = process.getInputStream(); boolean running = true; if (logger.isDebugEnabled()) &#123; logger.debug(logCommandOutput(is)); &#125; if (logger.isDebugEnabled()) &#123; logger.debug(logCommandOutput(process.getErrorStream())); &#125; while (running) &#123; try &#123; process.waitFor(); running = false; &#125; catch (InterruptedException e) &#123; // you could stop a process by interrupting it's Job thread process.destroy(); &#125; &#125; int exitVal = process.exitValue(); // if (logger.isDebugEnabled()) &#123; logger.debug("Exited with error code " + exitVal); &#125; failed = exitVal != 0; &#125; catch (Exception e) &#123; logger.error("Failed to convert template", e); failed = true; &#125; // end try-catch &#125; while (failed &amp;&amp; (retryCount &lt; MAX_RETRY_COUNT)); if (failed) &#123; throw new RuntimeException("Failed generating PDF file"); &#125; &#125; // end method convert /** * DOCUMENT ME! * * @param cmdPath DOCUMENT ME! */ public void setCmdPath(String cmdPath) &#123; this.cmdPath = cmdPath; &#125; /** * DOCUMENT ME! * * @param extraArgs DOCUMENT ME! */ public void setExtraArgs(String extraArgs) &#123; this.extraArgs = extraArgs; &#125; /** * DOCUMENT ME! * * @param input DOCUMENT ME! */ public void setInput(String input) &#123; this.input = input; &#125; /** * DOCUMENT ME! * * @param output DOCUMENT ME! */ public void setOutput(String output) &#123; this.output = output; &#125; /** * DOCUMENT ME! * * @param runAsArgs DOCUMENT ME! */ public void setRunAsArgs(String runAsArgs) &#123; this.runAsArgs = runAsArgs; &#125; /** * 输出执行WKHtmlToPdf命令过程中的log信息. * * @param is InputStream * * @return 输出执行WKHtmlToPdf命令过程中的log信息. * * @throws IOException */ private String logCommandOutput(InputStream is) throws IOException &#123; StringBuffer sb = new StringBuffer("Processing \n"); if (logger.isDebugEnabled()) &#123; BufferedReader input = new BufferedReader(new InputStreamReader(is, Charset.forName("UTF-8"))); String line = null; while ((line = input.readLine()) != null) &#123; sb.append(line).append("\n"); &#125; &#125; return sb.toString(); &#125; /** * 打印输出最终拼凑出来的脚本命令. * * @param processBuilder ProcessBuilder * * @return 打印输出最终拼凑出来的脚本命令. */ private String logExecutingCommand(ProcessBuilder processBuilder) &#123; List&lt;String&gt; commands = processBuilder.command(); StringBuffer sb = new StringBuffer("Executing command: "); String tab = " "; for (int i = 0; i &lt; commands.size(); i++) &#123; String command = commands.get(i); if (i == 0) &#123; sb.append(command); &#125; else &#123; sb.append(tab).append(command); &#125; &#125; return sb.toString(); &#125;&#125; // end class WKHtml2PdfConverter 测试 测试代码: 12345678910public static void main(String[] args) &#123; WKHtml2PdfConverter converter = new WKHtml2PdfConverter(); converter.cmdPath = &quot;/usr/local/bin/wkhtmltopdf&quot;; converter.input = &quot;https://www.google.com/&quot;; converter.output = &quot;/tmp/Downloads/5.pdf&quot;; converter.extraArgs = &quot;--quiet&quot;; converter.convert();&#125; 老版本wkhtmltopdf依赖于X11, 测试方法: 1234567891011121314public static void main(String[] args) &#123; WKHtml2PdfConverter converter = new WKHtml2PdfConverter(); /** * 老版本的WKHtmlToPdf是需要依赖X11的 */ converter.runAsArgs = &quot;/usr/bin/xvfb-run&quot;; converter.cmdPath = &quot;/usr/bin/wkhtmltopdf-amd64&quot;; converter.input = &quot;/tmp/a.html&quot;; converter.output = &quot;/tmp/a.pdf&quot;; converter.extraArgs = &quot;--use-xserver&quot;; converter.convert();&#125;]]></content>
      <categories>
        <category>tools</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>wkhtmltopdf</tag>
        <tag>converter</tag>
        <tag>html</tag>
        <tag>pdf</tag>
      </tags>
  </entry>
</search>
